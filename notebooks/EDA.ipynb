{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e63d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "DATA = ROOT / \"data\"\n",
    "sys.path.append(str(ROOT / \"functions\"))\n",
    "\n",
    "from categoric_functions import *\n",
    "from numeric_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f5a33",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfac5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total shape: (590540, 434)\n"
     ]
    }
   ],
   "source": [
    "train_path = DATA / \"train_merged.csv\"\n",
    "df = pd.read_csv(train_path, low_memory=False)\n",
    "print(f\"Total shape: {df.shape}\")\n",
    "\n",
    "df = df.sort_values('TransactionDT').reset_index(drop=True)\n",
    "split_idx = int(len(df) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f16a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage decreased to 505.96 Mb (67.7% reduction)\n",
      "Memory usage decreased to 128.29 Mb (67.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_df = df.iloc[:split_idx].copy()\n",
    "test_df = df.iloc[split_idx:].copy()\n",
    "\n",
    "train_df = reduce_mem_usage(train_df.copy())\n",
    "test_df = reduce_mem_usage(test_df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296afcd",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "Some features have more than 95% missing values. These are redundat for ml models. I am defining a threshold to determine whether features with missing values will remain in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a05b9447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 414 columns with missing values.\n",
      "There are 9 columns with missing percent > 95%\n",
      " Threshold : 95%\n",
      " Dropped feature num : 9\n"
     ]
    }
   ],
   "source": [
    "threshold = 95 \n",
    "high_missing = top_missing_cols(train_df, thresh=threshold)\n",
    "cols_to_drop = high_missing[high_missing['missing_percent'] > threshold]['col'].tolist()\n",
    "\n",
    "train_df = train_df.drop(columns=cols_to_drop)\n",
    "print(f\" Threshold : {threshold}%\\n Dropped feature num : {len(cols_to_drop)}\")\n",
    "\n",
    "test_df = test_df.drop(columns=[c for c in cols_to_drop if c in test_df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88687132",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e349f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution for 'isFraud':\n",
      "isFraud\n",
      "0    455833\n",
      "1     16599\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage:\n",
      "isFraud\n",
      "0    0.964865\n",
      "1    0.035135\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution for 'isFraud':\")\n",
    "print(train_df['isFraud'].value_counts())\n",
    "print(\"\\nPercentage:\")\n",
    "print(train_df['isFraud'].value_counts(normalize=True))\n",
    "\n",
    "# counts = train_df['isFraud'].value_counts()\n",
    "# percentages = train_df['isFraud'].value_counts(normalize=True) * 100\n",
    "\n",
    "# colors = ['green', 'red'] \n",
    "# bars = plt.bar(counts.index, counts.values, color=colors[:len(counts)])\n",
    "\n",
    "# plt.title('isFraud Class Frequencies')\n",
    "# plt.xlabel('isFraud')\n",
    "# plt.ylabel('Count')\n",
    "# plt.xticks([0, 1])\n",
    "\n",
    "# for bar, perc in zip(bars, percentages):\n",
    "#     plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.05, f'{perc:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804d2ef3",
   "metadata": {},
   "source": [
    "It is known which features are categorical. After cleaning up missing values, the remaining features need to be analysed.\n",
    "\n",
    "**Categorical features are initially divided into two groups**:\n",
    "* High cardinality -> 18 features\n",
    "* Ready for analysis -> 24 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6512a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'ProductCD', 'P_emaildomain', 'R_emaildomain','DeviceType', 'DeviceInfo',\n",
    "    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "    'addr1', 'addr2',\n",
    "    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "    'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20',\n",
    "    'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n",
    "    'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38'\n",
    "]\n",
    "numerical_features = [col for col in train_df.columns if col not in categorical_features]\n",
    "original_numerical_features = numerical_features.copy() \n",
    "present_cat_cols = list(set(categorical_features) & set(train_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e265fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinality_threshold = 15\n",
    "low_cardinality = [col for col in present_cat_cols if train_df[col].nunique() <= cardinality_threshold]\n",
    "high_cardinality = [col for col in present_cat_cols if train_df[col].nunique() > cardinality_threshold]\n",
    "\n",
    "categorical_features = low_cardinality\n",
    "cardinality_features = high_cardinality "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f872173",
   "metadata": {},
   "source": [
    "The functions that should be applied for categorical variables are ready, but I want to set them up as a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aef93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Main pipeline , rare_maps are containing all rare category mappings\n",
    "def apply_categorical_engineering(df, rare_maps=None):\n",
    "    \"\"\"\n",
    "    Complete categorical feature engineering pipeline.\n",
    "    functions chained together.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Rare encoding\n",
    "    df, rare_maps = encode_rare_categories(\n",
    "        df, \n",
    "        columns={'card3': 200, 'card5': 300},\n",
    "        rare_maps=rare_maps\n",
    "    )\n",
    "    \n",
    "    # Domain-specific transformations\n",
    "    df = (df\n",
    "        .pipe(clean_email_domains)\n",
    "        .pipe(create_email_match)\n",
    "        .pipe(consolidate_device_info)\n",
    "        .pipe(extract_screen_features)\n",
    "    )\n",
    "    return df, rare_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d766a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Rare category encoding (reusable) --> car3 ve card5 te kullanılacak.\n",
    "def encode_rare_categories(df, columns, thresh = 200, rare_maps=None):\n",
    "    \"\"\"\n",
    "    Replace rare categories with 'Others'.\n",
    "    \n",
    "    Args:\n",
    "        columns: dict {col: threshold} or list\n",
    "        rare_maps: dict {col: [rare_values]} for test set\n",
    "    \"\"\"\n",
    "    is_train = rare_maps is None\n",
    "    if is_train:\n",
    "        rare_maps = {}\n",
    "    \n",
    "    if isinstance(columns, list):\n",
    "        columns = {col: 200 for col in columns}\n",
    "    \n",
    "    for col, thresh in columns.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        if is_train:\n",
    "            counts = df[col].value_counts()\n",
    "            rare_maps[col] = counts[counts < thresh].index.tolist()\n",
    "        \n",
    "        df.loc[df[col].isin(rare_maps[col]), col] = 'Others'\n",
    "    \n",
    "    return df, rare_maps\n",
    "\n",
    "\n",
    "#  Screen resolution features\n",
    "def extract_screen_features(df):\n",
    "    \"\"\"Parse id_33 into width, height, pixels, aspect ratio.\"\"\"\n",
    "    if 'id_33' not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    split = df['id_33'].astype(str).str.split('x', expand=True)\n",
    "    if split.shape[1] != 2:\n",
    "        return df\n",
    "    \n",
    "    df['screen_width'] = pd.to_numeric(split[0], errors='coerce')\n",
    "    df['screen_height'] = pd.to_numeric(split[1], errors='coerce')\n",
    "    df['total_pixels'] = df['screen_width'] * df['screen_height']\n",
    "    df['aspect_ratio'] = (df['screen_width'] / df['screen_height']).round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_interaction_features(df, interactions, prefix='inter'):\n",
    "    \"\"\"\n",
    "    Create interaction features from predefined column pairs.\n",
    "    Safe for train/test - no data leakage.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        interactions: list of tuples [(col1, col2), ...]\n",
    "        prefix: prefix for new feature names\n",
    "    \n",
    "    Returns:\n",
    "        df with new interaction columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for col1, col2 in interactions:\n",
    "        if col1 in df.columns and col2 in df.columns:\n",
    "            new_name = f\"{prefix}_{col1}_x_{col2}\"\n",
    "            df[new_name] = (\n",
    "                df[col1].astype(str).fillna('missing') + '_' + \n",
    "                df[col2].astype(str).fillna('missing')\n",
    "            )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2881637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set - learn rare categories\n",
    "train_df, rare_maps = apply_categorical_engineering(train_df, rare_maps=None)\n",
    "\n",
    "# Test set - use learned rare categories\n",
    "test_df, _ = apply_categorical_engineering(test_df, rare_maps=rare_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "717ea2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test email features --> ilerleyen aşamalrda özellik ekleyerek ne kadar CV skoruna etki ettiğine bakabiliriz.\n",
    "# email_features = ['P_emaildomain_bin', 'R_emaildomain_bin', 'email_match']\n",
    "# for feat in email_features:\n",
    "#     auc = test_single_feature(train_df, feat)\n",
    "#     print(f\"{feat}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d50dcdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 171 feature pairs...\n",
      "Progress: 100/171 pairs processed...\n",
      "\n",
      "Analysis complete! Found 171 valid combinations.\n",
      "Top fraud rate: 98.4%\n"
     ]
    }
   ],
   "source": [
    "categorical_to_scan = [\n",
    "'P_emaildomain_bin', 'R_emaildomain_bin', 'email_match', 'OS_type', 'Device_name',\n",
    "'card5', 'card3', 'card6','card4', 'ProductCD','DeviceType', 'screen_width' , 'screen_height', 'total_pixels' , 'aspect_ratio'\n",
    ", 'id_28' , 'id_20', 'id_15','id_19'\n",
    "]\n",
    "\n",
    "# Step 1: Scan ONLY on train to identify top interactions\n",
    "top_combos = scan_all_bivariate_combinations(\n",
    "    train_df, \n",
    "    feature_list=categorical_to_scan,\n",
    "    target='isFraud',\n",
    "    min_samples=50,\n",
    "    top_n=20\n",
    ")\n",
    "\n",
    "# Step 2: Extract column pairs (without fraud rates)\n",
    "interactions = [\n",
    "    (row['feature1'], row['feature2']) \n",
    "    for _, row in top_combos.iterrows()\n",
    "]\n",
    "\n",
    "# Step 3: Apply SAME interactions to both train and test\n",
    "train_df = create_interaction_features(train_df, interactions)\n",
    "test_df = create_interaction_features(test_df, interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef2b9a",
   "metadata": {},
   "source": [
    "Some features have become unnecessary after the transformation, and this is certain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ad0c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original columns that were transformed\n",
    "drop_cols = [\n",
    "    'P_emaildomain',      # replaced by P_emaildomain_bin\n",
    "    'R_emaildomain',      # replaced by R_emaildomain_bin\n",
    "    'id_30',              # replaced by OS_type\n",
    "    'DeviceInfo',         # replaced by Device_name\n",
    "    'id_33',              # replaced by screen_width, screen_height, etc.\n",
    "]\n",
    "\n",
    "train_df = train_df.drop(columns=drop_cols, errors='ignore')\n",
    "test_df = test_df.drop(columns=drop_cols, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b9d7d",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c427ccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Düşük Kardinalite Object Sütunları:\n",
      "['ProductCD', 'card3', 'card4', 'card6', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_28', 'id_29', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'P_emaildomain_bin', 'R_emaildomain_bin', 'email_match', 'OS_type', 'Device_name']\n",
      "\n",
      "Yüksek Kardinalite Object Sütunları:\n",
      "['card5', 'id_31', 'inter_card3_x_id_19', 'inter_Device_name_x_card3', 'inter_R_emaildomain_bin_x_aspect_ratio', 'inter_R_emaildomain_bin_x_screen_height', 'inter_R_emaildomain_bin_x_total_pixels', 'inter_ProductCD_x_aspect_ratio', 'inter_ProductCD_x_total_pixels', 'inter_ProductCD_x_screen_height', 'inter_P_emaildomain_bin_x_screen_height', 'inter_P_emaildomain_bin_x_total_pixels', 'inter_P_emaildomain_bin_x_aspect_ratio', 'inter_P_emaildomain_bin_x_id_19', 'inter_R_emaildomain_bin_x_id_19', 'inter_aspect_ratio_x_id_28', 'inter_aspect_ratio_x_id_15', 'inter_email_match_x_screen_width', 'inter_email_match_x_total_pixels', 'inter_id_20_x_id_19', 'inter_total_pixels_x_id_28', 'inter_screen_height_x_id_15']\n"
     ]
    }
   ],
   "source": [
    "# train_df'teki object tipindeki sütunların listesini almak için:\n",
    "object_columns = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "object_columns = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "\n",
    "cardinality_threshold = 15\n",
    "\n",
    "# Düşük ve yüksek kardinaliteyi ayır\n",
    "low_cardinality_objects = [col for col in object_columns if train_df[col].nunique() <= cardinality_threshold]\n",
    "high_cardinality_objects = [col for col in object_columns if train_df[col].nunique() > cardinality_threshold]\n",
    "\n",
    "print(\"Düşük Kardinalite Object Sütunları:\")\n",
    "print(low_cardinality_objects)\n",
    "\n",
    "print(\"\\nYüksek Kardinalite Object Sütunları:\")\n",
    "print(high_cardinality_objects)\n",
    "\n",
    "# Frequency encoding\n",
    "train_df, freq_maps = apply_frequency_encoding(train_df, high_cardinality_objects, normalize=True)\n",
    "test_df, _ = apply_frequency_encoding(test_df, high_cardinality_objects, freq_dict=freq_maps)\n",
    "\n",
    "# Label encoding  \n",
    "train_df, label_encoders = apply_label_encoding(train_df, low_cardinality_objects)\n",
    "test_df, _ = apply_label_encoding(test_df, low_cardinality_objects, encoder_dict=label_encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8222ffee",
   "metadata": {},
   "source": [
    "#### Evaluate the feature importance (categoric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef0e8c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_single_feature(df, feature, target='isFraud'):\n",
    "    \"\"\"Quick AUC test for a single feature.\"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    valid_data = df[[feature, target]].dropna()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        valid_data[[feature]], valid_data[target], \n",
    "        test_size=0.3, random_state=42, stratify=valid_data[target]\n",
    "    )\n",
    "    \n",
    "    from lightgbm import LGBMClassifier\n",
    "    model = LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    pred = model.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29b36e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_emaildomain_bin: 0.5869\n",
      "R_emaildomain_bin: 0.6728\n",
      "email_match: 0.6567\n"
     ]
    }
   ],
   "source": [
    "# Test email features --> ilerleyen aşamalrda özellik ekleyerek ne kadar CV skoruna etki ettiğine bakabiliriz.\n",
    "email_features = ['P_emaildomain_bin', 'R_emaildomain_bin', 'email_match']\n",
    "for feat in email_features:\n",
    "    auc = test_single_feature(train_df, feat)\n",
    "    print(f\"{feat}: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03d3b4",
   "metadata": {},
   "source": [
    " # Numerical Features\n",
    "\n",
    " Elimizdeki sayısal özelliklerle ilgili en dikkat çekici şey tabi ki boyut :) Datasetin ham haline 383 den fazla sayısal özellik bulunmaktadır.\n",
    "\n",
    "Sayısal özelliklerin büyük çoğunluğu vesta engineering tarafından türetilen \"V_\" özellikleridir. İsimleri gizlenmiş olan bu özellikler için boyut indirgeme yöntemleri kullanılmalıdır.\n",
    "\n",
    "* group_by_missing_pattern ve test_feature_discrimination fonksiyonları kullanılacak bu iş için."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45a963c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ad1a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cols = [col for col in train_df[numerical_features].columns if col.startswith('V')] \n",
    "c_cols = [col for col in train_df[numerical_features].columns if col.startswith('C')]\n",
    "d_cols = [col for col in train_df[numerical_features].columns if col.startswith('D')]\n",
    "id_cols = [col for col in train_df[numerical_features].columns if col.startswith('id_')]\n",
    "\n",
    "grouped_cols = set(v_cols + c_cols + d_cols + id_cols) # bunlardan olmayanları bulmak için\n",
    "\n",
    "indep_cols = [col for col in train_df[numerical_features].columns if col not in grouped_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_missing = train_df[v_cols].isnull()\n",
    "\n",
    "# missing_rates = v_missing.mean().sort_values(ascending=False)\n",
    "\n",
    "# plt.figure(figsize=(20, 6))\n",
    "# missing_rates.plot(kind='bar')\n",
    "# plt.title('V Sütunları Missing Rates')\n",
    "# plt.ylabel('Missing Rate')\n",
    "# plt.xlabel('V Columns')\n",
    "# plt.axhline(0.5, color='red', linestyle='--', label='50% threshold')\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89493b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Representatives Summary\n",
      "--------------------------------\n",
      "Original V columns: 339\n",
      "Selected representatives: 14\n",
      "Reduction: 95.9%\n",
      "\n",
      "Final shapes:\n",
      "  Train: (472432, 124)\n",
      "  Test: (118108, 124)\n"
     ]
    }
   ],
   "source": [
    "v_pattern_groups = group_by_missing_pattern(train_df, v_cols)\n",
    "\n",
    "representative_v = []\n",
    "\n",
    "for pattern_id, info in v_pattern_groups.items():\n",
    "    group_cols = info['columns']\n",
    "\n",
    "    if len(group_cols) == 1:\n",
    "        representative_v.extend(group_cols)\n",
    "        continue\n",
    "\n",
    "    ks_res = test_feature_discrimination(train_df, group_cols, test='ks')\n",
    "\n",
    "    if not ks_res.empty:\n",
    "        best = ks_res.iloc[0]['Feature']\n",
    "        representative_v.append(best)\n",
    "\n",
    "print(\"Selected Representatives Summary\")\n",
    "print(\"--------------------------------\")\n",
    "print(f\"Original V columns: {len(v_cols)}\")\n",
    "print(f\"Selected representatives: {len(representative_v)}\")\n",
    "print(f\"Reduction: {(1 - len(representative_v)/len(v_cols)) * 100:.1f}%\")\n",
    "\n",
    "# V sütunlarını filtrele - HEM TRAIN HEM TEST\n",
    "cols_to_keep = train_df.columns.difference(v_cols).union(pd.Index(representative_v))\n",
    "train_df = train_df[cols_to_keep]\n",
    "test_df = test_df[[c for c in cols_to_keep if c in test_df.columns]]\n",
    "\n",
    "# v_cols listesini güncelle\n",
    "v_cols = [col for col in train_df.columns if col.startswith('V')]\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"  Train: {train_df.shape}\")\n",
    "print(f\"  Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f2919fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Time baseline first\n",
    "train_df = convert_dt_to_day(train_df)\n",
    "test_df = convert_dt_to_day(test_df)\n",
    "\n",
    "# 2. Create UID (needs D1 in original form!)\n",
    "train_df = create_uid(train_df, uid_cols=['card1', 'addr1', 'D1'])\n",
    "test_df = create_uid(test_df, uid_cols=['card1', 'addr1', 'D1'])\n",
    "\n",
    "# 3. NOW normalize D columns (after UID creation)\n",
    "train_df = normalize_d_columns(train_df)\n",
    "test_df = normalize_d_columns(test_df)\n",
    "\n",
    "# 4. Amount features\n",
    "train_df = extract_amt_decimal(train_df)\n",
    "test_df = extract_amt_decimal(test_df)\n",
    "\n",
    "# 5. Apply log transform to TransactionAmt\n",
    "train_df['TransactionAmt_log'] = np.log1p(train_df['TransactionAmt'])\n",
    "test_df['TransactionAmt_log'] = np.log1p(test_df['TransactionAmt'])\n",
    "\n",
    "# 6. C velocity features\n",
    "train_df = create_c_velocity_features(train_df)\n",
    "test_df = create_c_velocity_features(test_df)\n",
    "\n",
    "# 7. UID aggregations (IMPORTANT: only on train!)\n",
    "train_df = create_uid_aggregations(train_df, uid_col='uid')\n",
    "test_df = create_uid_aggregations(test_df, uid_col='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc06b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numerical features: 160\n"
     ]
    }
   ],
   "source": [
    "# Exclude non-numeric and ID columns\n",
    "exclude_cols = ['TransactionID', 'TransactionDT', 'isFraud', 'uid']\n",
    "\n",
    "\n",
    "numerical_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features = [col for col in numerical_features if col not in exclude_cols]\n",
    "\n",
    "print(f\"Total numerical features: {len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42a77d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total original numerical features: 101\n"
     ]
    }
   ],
   "source": [
    "# Orijinal sayısal sütunları isimlere göre seç\n",
    "original_numerical_features = [col for col in train_df.columns if col.startswith(('V', 'C', 'D', 'id_')) and col not in exclude_cols]\n",
    "print(f\"Total original numerical features: {len(original_numerical_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf68efad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Feature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Test_Stat",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P_Value",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Significance",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "n_fraud",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_normal",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Unique_Ratio_Fraud",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Unique_Ratio_Normal",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "98553624-2a76-407b-be1a-c55c75c61877",
       "rows": [
        [
         "0",
         "V258",
         "0.4613",
         "0.0",
         "***",
         "8174",
         "101761",
         "0.006",
         "0.0"
        ],
        [
         "1",
         "D5",
         "0.4414",
         "0.0",
         "***",
         "8569",
         "210018",
         "0.036",
         "0.003"
        ],
        [
         "2",
         "C13_C1_ratio",
         "0.4014",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.071",
         "0.008"
        ],
        [
         "3",
         "V52",
         "0.3835",
         "0.0",
         "***",
         "11537",
         "321086",
         "0.001",
         "0.0"
        ],
        [
         "4",
         "D8_normalized",
         "0.3785",
         "0.0",
         "***",
         "6247",
         "55363",
         "0.999",
         "0.994"
        ],
        [
         "5",
         "V201",
         "0.3716",
         "0.0",
         "***",
         "8891",
         "107774",
         "0.006",
         "0.0"
        ],
        [
         "6",
         "V199",
         "0.3696",
         "0.0",
         "***",
         "8863",
         "107636",
         "0.005",
         "0.0"
        ],
        [
         "7",
         "D3",
         "0.3684",
         "0.0",
         "***",
         "7598",
         "247182",
         "0.031",
         "0.002"
        ],
        [
         "8",
         "V140",
         "0.3594",
         "0.0",
         "***",
         "3018",
         "66686",
         "0.007",
         "0.001"
        ],
        [
         "9",
         "V94",
         "0.3579",
         "0.0",
         "***",
         "12950",
         "381689",
         "0.0",
         "0.0"
        ],
        [
         "10",
         "V34",
         "0.3428",
         "0.0",
         "***",
         "13310",
         "392695",
         "0.001",
         "0.0"
        ],
        [
         "11",
         "D2",
         "0.3318",
         "0.0",
         "***",
         "6296",
         "234873",
         "0.079",
         "0.003"
        ],
        [
         "12",
         "V74",
         "0.3241",
         "0.0",
         "***",
         "12825",
         "392717",
         "0.001",
         "0.0"
        ],
        [
         "13",
         "C4",
         "0.3236",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.012",
         "0.002"
        ],
        [
         "14",
         "D8",
         "0.3102",
         "0.0",
         "***",
         "6247",
         "55363",
         "0.18",
         "0.093"
        ],
        [
         "15",
         "D9_normalized",
         "0.3087",
         "0.0",
         "***",
         "6247",
         "55363",
         "0.999",
         "0.977"
        ],
        [
         "16",
         "id_36",
         "0.3071",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.0",
         "0.0"
        ],
        [
         "17",
         "id_15",
         "0.3058",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.0",
         "0.0"
        ],
        [
         "18",
         "id_28",
         "0.3058",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.0",
         "0.0"
        ],
        [
         "19",
         "id_29",
         "0.3058",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.0",
         "0.0"
        ],
        [
         "20",
         "id_35",
         "0.3058",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.0",
         "0.0"
        ],
        [
         "21",
         "id_37",
         "0.3058",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.0",
         "0.0"
        ],
        [
         "22",
         "id_38",
         "0.3058",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.0",
         "0.0"
        ],
        [
         "23",
         "DeviceType",
         "0.3057",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.0",
         "0.0"
        ],
        [
         "24",
         "C8",
         "0.3056",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.017",
         "0.002"
        ],
        [
         "25",
         "id_31",
         "0.3053",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.005",
         "0.0"
        ],
        [
         "26",
         "V303",
         "0.3047",
         "0.0",
         "***",
         "16597",
         "455823",
         "0.001",
         "0.0"
        ],
        [
         "27",
         "id_12",
         "0.3039",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.0",
         "0.0"
        ],
        [
         "28",
         "C10",
         "0.303",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.013",
         "0.002"
        ],
        [
         "29",
         "V222",
         "0.2996",
         "0.0",
         "***",
         "8795",
         "109225",
         "0.006",
         "0.001"
        ],
        [
         "30",
         "C9_uid_mean",
         "0.2977",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.063",
         "0.006"
        ],
        [
         "31",
         "V11",
         "0.2973",
         "0.0",
         "***",
         "4484",
         "221975",
         "0.001",
         "0.0"
        ],
        [
         "32",
         "D2_normalized",
         "0.296",
         "0.0",
         "***",
         "6296",
         "234873",
         "0.999",
         "0.997"
        ],
        [
         "33",
         "D15_uid_mean",
         "0.2958",
         "0.0",
         "***",
         "15333",
         "433855",
         "0.165",
         "0.017"
        ],
        [
         "34",
         "C12",
         "0.2898",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.017",
         "0.002"
        ],
        [
         "35",
         "D7",
         "0.2895",
         "0.0",
         "***",
         "4482",
         "25462",
         "0.047",
         "0.022"
        ],
        [
         "36",
         "C5",
         "0.2881",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.006",
         "0.001"
        ],
        [
         "37",
         "id_17",
         "0.2877",
         "0.0",
         "***",
         "8916",
         "107769",
         "0.003",
         "0.001"
        ],
        [
         "38",
         "C9_uid_std",
         "0.2791",
         "0.0",
         "***",
         "13963",
         "309364",
         "0.1",
         "0.017"
        ],
        [
         "39",
         "C9",
         "0.2781",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.005",
         "0.0"
        ],
        [
         "40",
         "D2_uid_mean",
         "0.2738",
         "0.0",
         "***",
         "13210",
         "366385",
         "0.051",
         "0.006"
        ],
        [
         "41",
         "id_16",
         "0.2728",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.0",
         "0.0"
        ],
        [
         "42",
         "C7",
         "0.2717",
         "0.0",
         "***",
         "16599",
         "455833",
         "0.011",
         "0.002"
        ],
        [
         "43",
         "D10",
         "0.2518",
         "0.0",
         "***",
         "13311",
         "392730",
         "0.043",
         "0.002"
        ],
        [
         "44",
         "D15",
         "0.2414",
         "0.0",
         "***",
         "12950",
         "381717",
         "0.046",
         "0.002"
        ],
        [
         "45",
         "D15_normalized",
         "0.2259",
         "0.0",
         "***",
         "12950",
         "381717",
         "0.999",
         "0.992"
        ],
        [
         "46",
         "D1_normalized",
         "0.2247",
         "0.0",
         "***",
         "16572",
         "454924",
         "0.999",
         "0.987"
        ],
        [
         "47",
         "id_01",
         "0.2228",
         "0.0",
         "***",
         "9099",
         "111365",
         "0.004",
         "0.001"
        ],
        [
         "48",
         "D15_uid_std",
         "0.2168",
         "0.0",
         "***",
         "12965",
         "292922",
         "0.186",
         "0.041"
        ],
        [
         "49",
         "D7_normalized",
         "0.211",
         "0.0",
         "***",
         "4482",
         "25462",
         "1.0",
         "0.999"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 101
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Test_Stat</th>\n",
       "      <th>P_Value</th>\n",
       "      <th>Significance</th>\n",
       "      <th>n_fraud</th>\n",
       "      <th>n_normal</th>\n",
       "      <th>Unique_Ratio_Fraud</th>\n",
       "      <th>Unique_Ratio_Normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V258</td>\n",
       "      <td>0.4613</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>***</td>\n",
       "      <td>8174</td>\n",
       "      <td>101761</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D5</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>***</td>\n",
       "      <td>8569</td>\n",
       "      <td>210018</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C13_C1_ratio</td>\n",
       "      <td>0.4014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>***</td>\n",
       "      <td>16599</td>\n",
       "      <td>455833</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V52</td>\n",
       "      <td>0.3835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>***</td>\n",
       "      <td>11537</td>\n",
       "      <td>321086</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D8_normalized</td>\n",
       "      <td>0.3785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>***</td>\n",
       "      <td>6247</td>\n",
       "      <td>55363</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>D2_uid_std</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>***</td>\n",
       "      <td>10775</td>\n",
       "      <td>216130</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>id_04</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.396785</td>\n",
       "      <td>ns</td>\n",
       "      <td>5645</td>\n",
       "      <td>48445</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>id_11</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.479335</td>\n",
       "      <td>ns</td>\n",
       "      <td>9036</td>\n",
       "      <td>108760</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>id_10</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.998398</td>\n",
       "      <td>ns</td>\n",
       "      <td>6247</td>\n",
       "      <td>55363</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>C3</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.954916</td>\n",
       "      <td>ns</td>\n",
       "      <td>16599</td>\n",
       "      <td>455833</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Feature  Test_Stat   P_Value Significance  n_fraud  n_normal  \\\n",
       "0             V258     0.4613  0.000000          ***     8174    101761   \n",
       "1               D5     0.4414  0.000000          ***     8569    210018   \n",
       "2     C13_C1_ratio     0.4014  0.000000          ***    16599    455833   \n",
       "3              V52     0.3835  0.000000          ***    11537    321086   \n",
       "4    D8_normalized     0.3785  0.000000          ***     6247     55363   \n",
       "..             ...        ...       ...          ...      ...       ...   \n",
       "96      D2_uid_std     0.0202  0.000458          ***    10775    216130   \n",
       "97           id_04     0.0126  0.396785           ns     5645     48445   \n",
       "98           id_11     0.0092  0.479335           ns     9036    108760   \n",
       "99           id_10     0.0051  0.998398           ns     6247     55363   \n",
       "100             C3     0.0040  0.954916           ns    16599    455833   \n",
       "\n",
       "     Unique_Ratio_Fraud  Unique_Ratio_Normal  \n",
       "0                 0.006                0.000  \n",
       "1                 0.036                0.003  \n",
       "2                 0.071                0.008  \n",
       "3                 0.001                0.000  \n",
       "4                 0.999                0.994  \n",
       "..                  ...                  ...  \n",
       "96                0.019                0.006  \n",
       "97                0.002                0.000  \n",
       "98                0.007                0.001  \n",
       "99                0.002                0.001  \n",
       "100               0.000                0.000  \n",
       "\n",
       "[101 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ks_results = test_feature_discrimination(\n",
    "    train_df,\n",
    "    columns=original_numerical_features,\n",
    "    target='isFraud',\n",
    "    test='ks',\n",
    "    min_samples=30\n",
    ")\n",
    "\n",
    "# Display top results\n",
    "display(ks_results.head(101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a45784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_high_correlation(df, features, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features.\n",
    "    Keeps the feature with higher fraud discrimination.\n",
    "    \"\"\"\n",
    "    corr_matrix = df[features].corr().abs()\n",
    "    \n",
    "    # Upper triangle to avoid duplicates\n",
    "    upper = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Find correlated pairs\n",
    "    to_drop = set()\n",
    "    \n",
    "    for column in upper.columns:\n",
    "        correlated = upper[column][upper[column] > threshold].index.tolist()\n",
    "        if correlated:\n",
    "            # Keep the one with better KS score\n",
    "            for corr_col in correlated:\n",
    "                ks_col = ks_results[ks_results['Feature'] == column]['Test_Stat'].values[0]\n",
    "                ks_corr = ks_results[ks_results['Feature'] == corr_col]['Test_Stat'].values[0]\n",
    "                \n",
    "                if ks_col < ks_corr:\n",
    "                    to_drop.add(column)\n",
    "                else:\n",
    "                    to_drop.add(corr_col)\n",
    "    \n",
    "    print(f\"Removing {len(to_drop)} highly correlated features (r > {threshold})\")\n",
    "    return [f for f in features if f not in to_drop]\n",
    "\n",
    "# Apply\n",
    "filtered_features = remove_high_correlation(\n",
    "    train_df, \n",
    "    strong_features + moderate_features, \n",
    "    threshold=0.95\n",
    ")\n",
    "\n",
    "def cap_outliers(df, columns, lower_percentile=1, upper_percentile=99):\n",
    "    \"\"\"Cap extreme outliers at percentiles.\"\"\"\n",
    "    for col in columns:\n",
    "        lower = df[col].quantile(lower_percentile / 100)\n",
    "        upper = df[col].quantile(upper_percentile / 100)\n",
    "        df[col] = df[col].clip(lower, upper)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3208d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
