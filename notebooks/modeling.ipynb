{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64459b7",
   "metadata": {},
   "source": [
    "# Fraud Detection - Model Training and Evaluation (5 Scenarios)\n",
    "\n",
    "This notebook implements a **systematic comparison of 5 scenarios** for the IEEE-CIS Fraud Detection task.\n",
    "\n",
    "## Key Design Decisions\n",
    "\n",
    "**No SMOTE**: All scenarios use **class weighting only** (no synthetic oversampling).\n",
    "\n",
    "**Fixed Evaluation Protocol**:\n",
    "- StratifiedKFold: n_splits=3, shuffle=True, random_state=42\n",
    "- Metrics: CV ROC-AUC, Test ROC-AUC, Test PR-AUC (AP), Youden threshold, Precision/Recall/F1\n",
    "\n",
    "## 5 Comparison Scenarios\n",
    "\n",
    "| # | Scenario | Missing Strategy | Feature Set | Purpose |\n",
    "|---|----------|-----------------|-------------|---------|\n",
    "| 1 | Baseline | Sentinel (-999) | Full (175) | Control experiment |\n",
    "| 2 | NaN Strategy | NaN (native) | Full (175) | Missing value handling comparison |\n",
    "| 3 | No Interactions | Sentinel (-999) | No inter_* features | Feature ablation |\n",
    "| 4 | Strong Only | Sentinel (-999) | strong_features + strong_cat | Aggressive reduction |\n",
    "| 5 | Strong + Moderate | Sentinel (-999) | strong + moderate features | Moderate reduction |\n",
    "\n",
    "## Models Evaluated (per scenario)\n",
    "\n",
    "**Base Models** (4):\n",
    "1. RandomForest - Bagging ensemble with class_weight='balanced'\n",
    "2. XGBoost - Gradient boosting with scale_pos_weight\n",
    "3. LightGBM - Leaf-wise growth with is_unbalance=True\n",
    "4. CatBoost - Ordered boosting with auto_class_weights='Balanced'\n",
    "\n",
    "**Stacking Variants** (3):\n",
    "5. Stacking_Weighted - CV-AUC-based weighted average\n",
    "6. Stacking_Logistic - LogisticRegression meta-learner (OOF-trained)\n",
    "7. Stacking_Ridge - L2-regularized LogisticRegression meta-learner (OOF-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96aaa5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Note: SMOTE/imblearn NOT imported - using class weights only\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Import Libraries\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, auc, average_precision_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Boosting models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Custom functions\n",
    "import sys\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT / \"functions\"))\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Note: SMOTE/imblearn NOT imported - using class weights only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0c973",
   "metadata": {},
   "source": [
    "## 1. Load Data & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "547fd62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded successfully!\n",
      "   Train shape: (472432, 195)\n",
      "   Test shape: (118108, 195)\n",
      "   Features for modeling: 175\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Preprocessed Data from EDA.ipynb\n",
    "# =============================================================================\n",
    "import pickle\n",
    "\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "# Load DataFrames\n",
    "train_df = pd.read_parquet(DATA / \"train_preprocessed.parquet\")\n",
    "test_df = pd.read_parquet(DATA / \"test_preprocessed.parquet\")\n",
    "\n",
    "# Load feature lists\n",
    "with open(DATA / \"feature_lists.pkl\", 'rb') as f:\n",
    "    feature_lists = pickle.load(f)\n",
    "\n",
    "filtered_features = feature_lists['filtered_features']\n",
    "categorical_features = feature_lists['categorical_for_model']\n",
    "strong_features = feature_lists['strong_features']\n",
    "moderate_features = feature_lists['moderate_features']\n",
    "\n",
    "print(f\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"   Train shape: {train_df.shape}\")\n",
    "print(f\"   Test shape: {test_df.shape}\")\n",
    "print(f\"   Features for modeling: {len(filtered_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a78b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Available feature list keys:\n",
      "   filtered_features: 175 features\n",
      "   strong_features: 51 features\n",
      "   moderate_features: 55 features\n",
      "   weak_features: 13 features\n",
      "   strong_cat: 67 features\n",
      "   moderate_cat: 2 features\n",
      "   weak_cat: 3 features\n",
      "   categorical_for_model: 69 features\n",
      "   numerical_for_model: 106 features\n",
      "\n",
      "‚úÖ Configuration loaded:\n",
      "   CV: StratifiedKFold, n_splits=3, shuffle=True, random_state=42\n",
      "   Strong categorical features: 67\n",
      "   Moderate categorical features: 2\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Fixed evaluation protocol for all scenarios\n",
    "# =============================================================================\n",
    "\n",
    "# Cross-validation settings (fixed for all scenarios)\n",
    "N_FOLDS = 3\n",
    "CV_RANDOM_STATE = 42\n",
    "\n",
    "# Load feature lists and examine available keys\n",
    "print(\"üìã Available feature list keys:\")\n",
    "for key in feature_lists.keys():\n",
    "    if isinstance(feature_lists[key], list):\n",
    "        print(f\"   {key}: {len(feature_lists[key])} features\")\n",
    "    else:\n",
    "        print(f\"   {key}: {type(feature_lists[key])}\")\n",
    "\n",
    "# Check for categorical feature lists\n",
    "strong_cat = feature_lists.get('strong_cat', [])\n",
    "moderate_cat = feature_lists.get('moderate_cat', [])\n",
    "\n",
    "print(f\"\\n‚úÖ Configuration loaded:\")\n",
    "print(f\"   CV: StratifiedKFold, n_splits={N_FOLDS}, shuffle=True, random_state={CV_RANDOM_STATE}\")\n",
    "print(f\"   Strong categorical features: {len(strong_cat)}\")\n",
    "print(f\"   Moderate categorical features: {len(moderate_cat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a80d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Base Data prepared:\n",
      "   X_train_raw shape: (472432, 175)\n",
      "   X_test_raw shape: (118108, 175)\n",
      "   Train fraud rate: 3.51%\n",
      "   Test fraud rate: 3.44%\n",
      "   Class imbalance ratio (scale_pos_weight): 27.46\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Prepare Base Features and Target (Original Data - Scenarios will transform)\n",
    "# =============================================================================\n",
    "\n",
    "# Load original preprocessed data (with NaN values intact for scenario flexibility)\n",
    "X_train_raw = train_df[filtered_features].copy()\n",
    "y_train = train_df['isFraud'].copy()\n",
    "X_test_raw = test_df[filtered_features].copy()\n",
    "y_test = test_df['isFraud'].copy()\n",
    "\n",
    "# =============================================================================\n",
    "# Handle Infinity Values Only (NaN handling is scenario-specific)\n",
    "# =============================================================================\n",
    "inf_cols = X_train_raw.columns[X_train_raw.isin([np.inf, -np.inf]).any()].tolist()\n",
    "if inf_cols:\n",
    "    print(f\"‚ö†Ô∏è Found infinity in {len(inf_cols)} columns: {inf_cols}\")\n",
    "\n",
    "X_train_raw = X_train_raw.replace([np.inf, -np.inf], np.nan)\n",
    "X_test_raw = X_test_raw.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Calculate class imbalance ratio for class weighting\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "print(f\"\\nüìä Base Data prepared:\")\n",
    "print(f\"   X_train_raw shape: {X_train_raw.shape}\")\n",
    "print(f\"   X_test_raw shape: {X_test_raw.shape}\")\n",
    "print(f\"   Train fraud rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"   Test fraud rate: {y_test.mean()*100:.2f}%\")\n",
    "print(f\"   Class imbalance ratio (scale_pos_weight): {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b72aa",
   "metadata": {},
   "source": [
    "### Temporal Split Justification\n",
    "\n",
    "**Why Temporal vs Random Split:**\n",
    "\n",
    "In fraud detection, using a **temporal (time-based) split** is critical because:\n",
    "\n",
    "1. **Concept Drift**: Fraudsters continuously adapt their strategies over time, making historical patterns less predictive of future fraud.\n",
    "2. **Data Leakage Prevention**: Random splits would allow \"future\" information to leak into training, artificially inflating performance metrics.\n",
    "3. **Realistic Evaluation**: Models trained on past data must predict future fraud patterns, mirroring real-world deployment.\n",
    "\n",
    "**Academic Justification:**\n",
    "\n",
    "- *Krawczyk et al. (2017), \"Ensemble learning for data stream analysis: A survey\", Information Fusion*: Highlights the importance of temporal ordering in streaming data classification tasks.\n",
    "- *Dal Pozzolo et al. (2014), \"Learned lessons in credit card fraud detection from a practitioner perspective\", Expert Systems with Applications*: Demonstrates that temporal splits provide more realistic performance estimates in fraud detection.\n",
    "\n",
    "**Implication**: CV scores (computed on training data) may differ from test set performance because the test set represents a **future time period** with potentially different fraud patterns. This is expected behavior and reflects real-world deployment conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbba86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # Visualize Train/Test Split (Temporal)\n",
    "# # =============================================================================\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# # 1. Dataset Size Comparison\n",
    "# ax1 = axes[0]\n",
    "# sizes = [len(X_train), len(X_test)]\n",
    "# labels = ['Train (80%)', 'Test (20%)']\n",
    "# colors = ['#3498db', '#e74c3c']\n",
    "# bars = ax1.bar(labels, sizes, color=colors, edgecolor='black', linewidth=1.2)\n",
    "# ax1.set_ylabel('Number of Samples', fontsize=11)\n",
    "# ax1.set_title('Dataset Split', fontsize=12, fontweight='bold')\n",
    "# for bar, size in zip(bars, sizes):\n",
    "#     ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
    "#              f'{size:,}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# # 2. Fraud Rate Comparison\n",
    "# ax2 = axes[1]\n",
    "# fraud_rates = [y_train.mean()*100, y_test.mean()*100]\n",
    "# bars2 = ax2.bar(labels, fraud_rates, color=colors, edgecolor='black', linewidth=1.2)\n",
    "# ax2.set_ylabel('Fraud Rate (%)', fontsize=11)\n",
    "# ax2.set_title('Fraud Rate by Split', fontsize=12, fontweight='bold')\n",
    "# ax2.axhline(y=y_train.mean()*100, color='gray', linestyle='--', alpha=0.5, label='Train Rate')\n",
    "# for bar, rate in zip(bars2, fraud_rates):\n",
    "#     ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "#              f'{rate:.2f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# # 3. Class Distribution (Stacked)\n",
    "# ax3 = axes[2]\n",
    "# train_counts = [len(y_train) - y_train.sum(), y_train.sum()]\n",
    "# test_counts = [len(y_test) - y_test.sum(), y_test.sum()]\n",
    "# x_pos = np.arange(2)\n",
    "# width = 0.35\n",
    "\n",
    "# bars_normal = ax3.bar(x_pos - width/2, [train_counts[0], test_counts[0]], width, \n",
    "#                        label='Normal', color='#2ecc71', edgecolor='black')\n",
    "# bars_fraud = ax3.bar(x_pos + width/2, [train_counts[1], test_counts[1]], width, \n",
    "#                       label='Fraud', color='#e74c3c', edgecolor='black')\n",
    "# ax3.set_xticks(x_pos)\n",
    "# ax3.set_xticklabels(['Train', 'Test'])\n",
    "# ax3.set_ylabel('Count', fontsize=11)\n",
    "# ax3.set_title('Class Distribution', fontsize=12, fontweight='bold')\n",
    "# ax3.legend(loc='upper right')\n",
    "# ax3.set_yscale('log')  # Log scale due to imbalance\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Summary table\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"DATA SPLIT SUMMARY (Temporal Split: Train=Past, Test=Future)\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"{'Dataset':<12} {'Samples':>12} {'Normal':>12} {'Fraud':>10} {'Fraud %':>10}\")\n",
    "# print(\"-\"*60)\n",
    "# print(f\"{'Train':<12} {len(X_train):>12,} {int(len(y_train)-y_train.sum()):>12,} {int(y_train.sum()):>10,} {y_train.mean()*100:>9.2f}%\")\n",
    "# print(f\"{'Test':<12} {len(X_test):>12,} {int(len(y_test)-y_test.sum()):>12,} {int(y_test.sum()):>10,} {y_test.mean()*100:>9.2f}%\")\n",
    "# print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e89c0",
   "metadata": {},
   "source": [
    "## 2. Model Definitions (Class-Weighted Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38f0d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model factory functions defined (class-weighted only, no SMOTE)\n",
      "   Class imbalance ratio for XGBoost: 27.46\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Model Factory Functions (Class-weighted only - No SMOTE)\n",
    "# =============================================================================\n",
    "\n",
    "def create_random_forest(use_imputer: bool = False):\n",
    "    \"\"\"\n",
    "    Create RandomForest classifier with optional imputation for NaN handling.\n",
    "    \n",
    "    Args:\n",
    "        use_imputer: If True, wrap RF in pipeline with median imputer (for NaN strategy)\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    if use_imputer:\n",
    "        return Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('classifier', rf)\n",
    "        ])\n",
    "    return rf\n",
    "\n",
    "\n",
    "def create_xgboost(scale_pos_weight: float):\n",
    "    \"\"\"Create XGBoost classifier with class weighting.\"\"\"\n",
    "    return XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='auc',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "\n",
    "def create_lightgbm():\n",
    "    \"\"\"Create LightGBM classifier with is_unbalance flag.\"\"\"\n",
    "    return LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        is_unbalance=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "\n",
    "def create_catboost():\n",
    "    \"\"\"Create CatBoost classifier with auto_class_weights.\"\"\"\n",
    "    return CatBoostClassifier(\n",
    "        iterations=300,\n",
    "        depth=6,\n",
    "        learning_rate=0.05,\n",
    "        auto_class_weights='Balanced',\n",
    "        eval_metric='AUC',\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "\n",
    "def get_base_models(missing_strategy: str, scale_pos_weight: float):\n",
    "    \"\"\"\n",
    "    Get dictionary of base models configured for the given missing strategy.\n",
    "    \n",
    "    Args:\n",
    "        missing_strategy: 'sentinel' or 'nan'\n",
    "        scale_pos_weight: Class imbalance ratio for XGBoost\n",
    "    \n",
    "    Returns:\n",
    "        Dict of model_name -> model instance\n",
    "    \"\"\"\n",
    "    use_imputer = (missing_strategy == 'nan')  # RF needs imputer for NaN\n",
    "    \n",
    "    return {\n",
    "        'RandomForest': create_random_forest(use_imputer=use_imputer),\n",
    "        'XGBoost': create_xgboost(scale_pos_weight),\n",
    "        'LightGBM': create_lightgbm(),\n",
    "        'CatBoost': create_catboost()\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Model factory functions defined (class-weighted only, no SMOTE)\")\n",
    "print(f\"   Class imbalance ratio for XGBoost: {scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae8680",
   "metadata": {},
   "source": [
    "## Scenario Definitions\n",
    "\n",
    "We define **5 comparison scenarios** to systematically evaluate different feature engineering and missing value strategies.\n",
    "\n",
    "| Scenario | Missing Strategy | Feature Set | Description |\n",
    "|----------|-----------------|-------------|-------------|\n",
    "| 1 | Sentinel (-999) | Full (175 features) | Baseline control |\n",
    "| 2 | NaN (native) | Full (175 features) | Compare missing value handling |\n",
    "| 3 | Sentinel (-999) | No interaction features | Feature ablation study |\n",
    "| 4 | Sentinel (-999) | Strong features only | Aggressive feature reduction |\n",
    "| 5 | Sentinel (-999) | Strong + Moderate features | Moderate feature reduction |\n",
    "\n",
    "**All scenarios use:**\n",
    "- Class weighting mechanisms (no SMOTE)\n",
    "- StratifiedKFold: n_splits=3, shuffle=True, random_state=42\n",
    "- Metrics: CV ROC-AUC, Test ROC-AUC, Test PR-AUC, Youden threshold, Precision/Recall/F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114c1664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Feature subset sizes:\n",
      "   Full features: 175\n",
      "   Interaction features (inter_*): 30\n",
      "   Non-interaction features: 145\n",
      "   Strong (num + cat): 118\n",
      "   Strong + Moderate: 175\n",
      "\n",
      "‚úÖ Scenarios defined:\n",
      "   Scenario 1: Baseline (Control) (175 features, missing=sentinel)\n",
      "   Scenario 2: NaN Strategy (175 features, missing=nan)\n",
      "   Scenario 3: No Interaction Features (145 features, missing=sentinel)\n",
      "   Scenario 4: Strong Features Only (118 features, missing=sentinel)\n",
      "   Scenario 5: Strong + Moderate Features (175 features, missing=sentinel)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Define All 5 Scenarios\n",
    "# =============================================================================\n",
    "\n",
    "# Get available feature subsets\n",
    "interaction_features = [f for f in filtered_features if f.startswith('inter_')]\n",
    "non_interaction_features = [f for f in filtered_features if not f.startswith('inter_')]\n",
    "\n",
    "# Strong features (numerical + categorical)\n",
    "strong_num = feature_lists.get('strong_features', [])\n",
    "strong_cat = feature_lists.get('strong_cat', [])\n",
    "strong_all = strong_num + strong_cat\n",
    "strong_all = [f for f in strong_all if f in X_train_raw.columns]\n",
    "\n",
    "# Moderate features (numerical + categorical if available)\n",
    "moderate_num = feature_lists.get('moderate_features', [])\n",
    "moderate_cat = feature_lists.get('moderate_cat', [])\n",
    "\n",
    "# Strong + Moderate combined\n",
    "strong_moderate_all = strong_num + moderate_num + strong_cat + moderate_cat\n",
    "strong_moderate_all = list(set(strong_moderate_all))  # Remove duplicates\n",
    "strong_moderate_all = [f for f in strong_moderate_all if f in X_train_raw.columns]\n",
    "\n",
    "print(\"üìä Feature subset sizes:\")\n",
    "print(f\"   Full features: {len(filtered_features)}\")\n",
    "print(f\"   Interaction features (inter_*): {len(interaction_features)}\")\n",
    "print(f\"   Non-interaction features: {len(non_interaction_features)}\")\n",
    "print(f\"   Strong (num + cat): {len(strong_all)}\")\n",
    "print(f\"   Strong + Moderate: {len(strong_moderate_all)}\")\n",
    "if not moderate_cat:\n",
    "    print(\"   ‚ö†Ô∏è Note: moderate_cat not found in feature_lists.pkl, using empty list\")\n",
    "\n",
    "# =============================================================================\n",
    "# Define Scenario Configurations\n",
    "# =============================================================================\n",
    "SCENARIOS = {\n",
    "    1: {\n",
    "        'name': 'Baseline (Control)',\n",
    "        'missing_strategy': 'sentinel',\n",
    "        'features': filtered_features.copy(),\n",
    "        'description': 'Missing=-999, full 175 features'\n",
    "    },\n",
    "    2: {\n",
    "        'name': 'NaN Strategy',\n",
    "        'missing_strategy': 'nan',\n",
    "        'features': filtered_features.copy(),\n",
    "        'description': 'Missing=NaN, full 175 features'\n",
    "    },\n",
    "    3: {\n",
    "        'name': 'No Interaction Features',\n",
    "        'missing_strategy': 'sentinel',\n",
    "        'features': non_interaction_features.copy(),\n",
    "        'description': 'Missing=-999, no inter_* features'\n",
    "    },\n",
    "    4: {\n",
    "        'name': 'Strong Features Only',\n",
    "        'missing_strategy': 'sentinel',\n",
    "        'features': strong_all.copy(),\n",
    "        'description': 'Missing=-999, strong_features + strong_cat'\n",
    "    },\n",
    "    5: {\n",
    "        'name': 'Strong + Moderate Features',\n",
    "        'missing_strategy': 'sentinel',\n",
    "        'features': strong_moderate_all.copy(),\n",
    "        'description': 'Missing=-999, strong + moderate features'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Scenarios defined:\")\n",
    "for sid, cfg in SCENARIOS.items():\n",
    "    print(f\"   Scenario {sid}: {cfg['name']} ({len(cfg['features'])} features, missing={cfg['missing_strategy']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5535b5f",
   "metadata": {},
   "source": [
    "## 3. Scenario Runner - Model Training and Evaluation\n",
    "\n",
    "**For each scenario, the runner:**\n",
    "1. Prepares features according to scenario configuration\n",
    "2. Trains 4 base models (RF, XGB, LGBM, CatBoost) with 3-fold CV\n",
    "3. Collects out-of-fold (OOF) predictions for stacking\n",
    "4. Builds 3 stacking variants:\n",
    "   - **Stacking_Weighted**: CV-AUC-based weighted average\n",
    "   - **Stacking_Logistic**: LogisticRegression meta-learner on OOF\n",
    "   - **Stacking_Ridge**: L2-regularized LogisticRegression meta-learner\n",
    "5. Evaluates all 7 approaches on the test set\n",
    "6. Records metrics: CV_AUC, Test_AUC, Test_AP, Youden threshold, Precision/Recall/F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a4fa43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Scenario Runner - All 5 Scenarios\n",
      "   CV: StratifiedKFold, n_splits=3, shuffle=True, random_state=42\n",
      "   Models: RandomForest, XGBoost, LightGBM, CatBoost + 3 Stacking variants\n",
      "\n",
      "================================================================================\n",
      "SCENARIO 1: Baseline (Control)\n",
      "Description: Missing=-999, full 175 features\n",
      "================================================================================\n",
      "Data: 175 features, missing=sentinel\n",
      "  RandomForest: CV AUC 0.9345¬±0.0006 | Test AUC 0.8845 | 260.2s\n",
      "  XGBoost: CV AUC 0.9404¬±0.0003 | Test AUC 0.8978 | 443.4s\n",
      "  LightGBM: CV AUC 0.9381¬±0.0008 | Test AUC 0.9003 | 41.3s\n",
      "  CatBoost: CV AUC 0.9182¬±0.0008 | Test AUC 0.8933 | 113.8s\n",
      "  Stacking_Weighted: Test AUC 0.9027\n",
      "  Stacking_Logistic: Test AUC 0.8732\n",
      "  Stacking_Ridge: Test AUC 0.8734\n",
      "\n",
      "================================================================================\n",
      "SCENARIO 2: NaN Strategy\n",
      "Description: Missing=NaN, full 175 features\n",
      "================================================================================\n",
      "Data: 175 features, missing=nan\n",
      "  RandomForest: CV AUC 0.9336¬±0.0004 | Test AUC 0.8889 | 221.5s\n",
      "  XGBoost: CV AUC 0.9410¬±0.0001 | Test AUC 0.9047 | 447.9s\n",
      "  LightGBM: CV AUC 0.9396¬±0.0015 | Test AUC 0.9075 | 36.1s\n",
      "  CatBoost: CV AUC 0.9179¬±0.0015 | Test AUC 0.8883 | 116.2s\n",
      "  Stacking_Weighted: Test AUC 0.9053\n",
      "  Stacking_Logistic: Test AUC 0.8936\n",
      "  Stacking_Ridge: Test AUC 0.8937\n",
      "\n",
      "================================================================================\n",
      "SCENARIO 3: No Interaction Features\n",
      "Description: Missing=-999, no inter_* features\n",
      "================================================================================\n",
      "Data: 145 features, missing=sentinel\n",
      "  RandomForest: CV AUC 0.9307¬±0.0008 | Test AUC 0.8807 | 204.3s\n",
      "  XGBoost: CV AUC 0.9403¬±0.0008 | Test AUC 0.9008 | 389.2s\n",
      "  LightGBM: CV AUC 0.9380¬±0.0014 | Test AUC 0.9000 | 38.2s\n",
      "  CatBoost: CV AUC 0.9184¬±0.0010 | Test AUC 0.8894 | 114.3s\n",
      "  Stacking_Weighted: Test AUC 0.9018\n",
      "  Stacking_Logistic: Test AUC 0.8734\n",
      "  Stacking_Ridge: Test AUC 0.8736\n",
      "\n",
      "================================================================================\n",
      "SCENARIO 4: Strong Features Only\n",
      "Description: Missing=-999, strong_features + strong_cat\n",
      "================================================================================\n",
      "Data: 118 features, missing=sentinel\n",
      "  RandomForest: CV AUC 0.9298¬±0.0011 | Test AUC 0.8820 | 160.4s\n",
      "  XGBoost: CV AUC 0.9345¬±0.0004 | Test AUC 0.8974 | 310.4s\n",
      "  LightGBM: CV AUC 0.9318¬±0.0008 | Test AUC 0.8998 | 26.4s\n",
      "  CatBoost: CV AUC 0.9110¬±0.0009 | Test AUC 0.8863 | 98.6s\n",
      "  Stacking_Weighted: Test AUC 0.8987\n",
      "  Stacking_Logistic: Test AUC 0.8836\n",
      "  Stacking_Ridge: Test AUC 0.8836\n",
      "\n",
      "================================================================================\n",
      "SCENARIO 5: Strong + Moderate Features\n",
      "Description: Missing=-999, strong + moderate features\n",
      "================================================================================\n",
      "Data: 175 features, missing=sentinel\n",
      "  RandomForest: CV AUC 0.9343¬±0.0004 | Test AUC 0.8864 | 224.8s\n",
      "  XGBoost: CV AUC 0.9404¬±0.0009 | Test AUC 0.8977 | 481.5s\n",
      "  LightGBM: CV AUC 0.9384¬±0.0012 | Test AUC 0.9014 | 39.8s\n",
      "  CatBoost: CV AUC 0.9182¬±0.0010 | Test AUC 0.8904 | 128.0s\n",
      "  Stacking_Weighted: Test AUC 0.9023\n",
      "  Stacking_Logistic: Test AUC 0.8754\n",
      "  Stacking_Ridge: Test AUC 0.8757\n",
      "\n",
      "================================================================================\n",
      "‚úÖ All scenarios complete! Total results: 35 rows\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Scenario Runner - Execute All Scenarios End-to-End\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data_for_scenario(X_train_raw, X_test_raw, features, missing_strategy):\n",
    "    \"\"\"Prepare train/test data for a specific scenario.\"\"\"\n",
    "    X_train = X_train_raw[features].copy()\n",
    "    X_test = X_test_raw[features].copy()\n",
    "    \n",
    "    if missing_strategy == 'sentinel':\n",
    "        MISSING_FLAG = -999\n",
    "        X_train = X_train.fillna(MISSING_FLAG)\n",
    "        X_test = X_test.fillna(MISSING_FLAG)\n",
    "    # For 'nan', keep NaN values (handled by models or imputers)\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def evaluate_model_on_test(y_true, y_prob):\n",
    "    \"\"\"Calculate all test metrics including Youden-optimal threshold.\"\"\"\n",
    "    test_auc = roc_auc_score(y_true, y_prob)\n",
    "    test_ap = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    # Optimal threshold using Youden's J statistic\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_thresh = thresholds[optimal_idx]\n",
    "    \n",
    "    # Metrics at optimal threshold\n",
    "    y_pred_binary = (y_prob >= optimal_thresh).astype(int)\n",
    "    precision_opt = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall_opt = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    f1_opt = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'Test_AUC': test_auc,\n",
    "        'Test_AP': test_ap,\n",
    "        'Optimal_Threshold': optimal_thresh,\n",
    "        'Precision_opt': precision_opt,\n",
    "        'Recall_opt': recall_opt,\n",
    "        'F1_opt': f1_opt\n",
    "    }\n",
    "\n",
    "\n",
    "def run_scenario(scenario_id, config, X_train_raw, X_test_raw, y_train, y_test, \n",
    "                 scale_pos_weight, n_folds=3, random_state=42):\n",
    "    \"\"\"\n",
    "    Run a complete scenario: train base models with CV, build stackings, evaluate on test.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results for all 7 approaches (4 base + 3 stacking)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SCENARIO {scenario_id}: {config['name']}\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Prepare data for this scenario\n",
    "    X_train, X_test = prepare_data_for_scenario(\n",
    "        X_train_raw, X_test_raw, config['features'], config['missing_strategy']\n",
    "    )\n",
    "    print(f\"Data: {X_train.shape[1]} features, missing={config['missing_strategy']}\")\n",
    "    \n",
    "    # Get models for this scenario\n",
    "    base_models = get_base_models(config['missing_strategy'], scale_pos_weight)\n",
    "    \n",
    "    # Initialize storage\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    results = []\n",
    "    oof_predictions = {}\n",
    "    test_predictions = {}\n",
    "    cv_scores_dict = {}\n",
    "    \n",
    "    # Train each base model with CV + OOF collection\n",
    "    for model_name, model in base_models.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        oof_probs = np.zeros(len(X_train))\n",
    "        cv_scores = []\n",
    "        \n",
    "        # K-Fold CV with OOF prediction collection\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "            X_fold_train = X_train.iloc[train_idx]\n",
    "            y_fold_train = y_train.iloc[train_idx]\n",
    "            X_fold_val = X_train.iloc[val_idx]\n",
    "            y_fold_val = y_train.iloc[val_idx]\n",
    "            \n",
    "            fold_model = clone(model)\n",
    "            fold_model.fit(X_fold_train, y_fold_train)\n",
    "            \n",
    "            fold_probs = fold_model.predict_proba(X_fold_val)[:, 1]\n",
    "            oof_probs[val_idx] = fold_probs\n",
    "            \n",
    "            fold_auc = roc_auc_score(y_fold_val, fold_probs)\n",
    "            cv_scores.append(fold_auc)\n",
    "        \n",
    "        cv_scores = np.array(cv_scores)\n",
    "        oof_predictions[model_name] = oof_probs\n",
    "        cv_scores_dict[model_name] = cv_scores.mean()\n",
    "        \n",
    "        # Train final model on full training data\n",
    "        final_model = clone(model)\n",
    "        final_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Test predictions\n",
    "        test_probs = final_model.predict_proba(X_test)[:, 1]\n",
    "        test_predictions[model_name] = test_probs\n",
    "        \n",
    "        # Evaluate on test\n",
    "        test_metrics = evaluate_model_on_test(y_test, test_probs)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'Scenario': scenario_id,\n",
    "            'Approach': model_name,\n",
    "            'CV_AUC_mean': cv_scores.mean(),\n",
    "            'CV_AUC_std': cv_scores.std(),\n",
    "            **test_metrics\n",
    "        })\n",
    "        \n",
    "        print(f\"  {model_name}: CV AUC {cv_scores.mean():.4f}¬±{cv_scores.std():.4f} | \"\n",
    "              f\"Test AUC {test_metrics['Test_AUC']:.4f} | {elapsed:.1f}s\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # Build Stacking Variants\n",
    "    # ==========================================================================\n",
    "    model_names = list(base_models.keys())\n",
    "    oof_matrix = np.column_stack([oof_predictions[name] for name in model_names])\n",
    "    test_matrix = np.column_stack([test_predictions[name] for name in model_names])\n",
    "    \n",
    "    # Scale for meta-learners\n",
    "    meta_scaler = StandardScaler()\n",
    "    oof_matrix_scaled = meta_scaler.fit_transform(oof_matrix)\n",
    "    test_matrix_scaled = meta_scaler.transform(test_matrix)\n",
    "    \n",
    "    # --- Stacking_Weighted: CV-AUC-based weights ---\n",
    "    total_auc = sum(cv_scores_dict.values())\n",
    "    weights = np.array([cv_scores_dict[name] / total_auc for name in model_names])\n",
    "    \n",
    "    weighted_test_probs = np.dot(test_matrix, weights)\n",
    "    test_predictions['Stacking_Weighted'] = weighted_test_probs\n",
    "    test_metrics = evaluate_model_on_test(y_test, weighted_test_probs)\n",
    "    \n",
    "    results.append({\n",
    "        'Scenario': scenario_id,\n",
    "        'Approach': 'Stacking_Weighted',\n",
    "        'CV_AUC_mean': np.nan,\n",
    "        'CV_AUC_std': np.nan,\n",
    "        **test_metrics\n",
    "    })\n",
    "    print(f\"  Stacking_Weighted: Test AUC {test_metrics['Test_AUC']:.4f}\")\n",
    "    \n",
    "    # --- Stacking_Logistic: OOF-trained LogisticRegression ---\n",
    "    meta_logistic = LogisticRegression(\n",
    "        C=1.0, class_weight='balanced', solver='lbfgs', max_iter=1000, random_state=42\n",
    "    )\n",
    "    meta_logistic.fit(oof_matrix_scaled, y_train)\n",
    "    \n",
    "    logistic_test_probs = meta_logistic.predict_proba(test_matrix_scaled)[:, 1]\n",
    "    test_predictions['Stacking_Logistic'] = logistic_test_probs\n",
    "    test_metrics = evaluate_model_on_test(y_test, logistic_test_probs)\n",
    "    \n",
    "    results.append({\n",
    "        'Scenario': scenario_id,\n",
    "        'Approach': 'Stacking_Logistic',\n",
    "        'CV_AUC_mean': np.nan,\n",
    "        'CV_AUC_std': np.nan,\n",
    "        **test_metrics\n",
    "    })\n",
    "    print(f\"  Stacking_Logistic: Test AUC {test_metrics['Test_AUC']:.4f}\")\n",
    "    \n",
    "    # --- Stacking_Ridge: L2-regularized LogisticRegression (Ridge-like) ---\n",
    "    meta_ridge = LogisticRegression(\n",
    "        C=0.1, penalty='l2', class_weight='balanced', solver='lbfgs', \n",
    "        max_iter=1000, random_state=42\n",
    "    )\n",
    "    meta_ridge.fit(oof_matrix_scaled, y_train)\n",
    "    \n",
    "    ridge_test_probs = meta_ridge.predict_proba(test_matrix_scaled)[:, 1]\n",
    "    test_predictions['Stacking_Ridge'] = ridge_test_probs\n",
    "    test_metrics = evaluate_model_on_test(y_test, ridge_test_probs)\n",
    "    \n",
    "    results.append({\n",
    "        'Scenario': scenario_id,\n",
    "        'Approach': 'Stacking_Ridge',\n",
    "        'CV_AUC_mean': np.nan,\n",
    "        'CV_AUC_std': np.nan,\n",
    "        **test_metrics\n",
    "    })\n",
    "    print(f\"  Stacking_Ridge: Test AUC {test_metrics['Test_AUC']:.4f}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Execute All Scenarios\n",
    "# =============================================================================\n",
    "print(\"üöÄ Starting Scenario Runner - All 5 Scenarios\")\n",
    "print(f\"   CV: StratifiedKFold, n_splits={N_FOLDS}, shuffle=True, random_state={CV_RANDOM_STATE}\")\n",
    "print(f\"   Models: RandomForest, XGBoost, LightGBM, CatBoost + 3 Stacking variants\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for scenario_id, config in SCENARIOS.items():\n",
    "    scenario_results = run_scenario(\n",
    "        scenario_id=scenario_id,\n",
    "        config=config,\n",
    "        X_train_raw=X_train_raw,\n",
    "        X_test_raw=X_test_raw,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_folds=N_FOLDS,\n",
    "        random_state=CV_RANDOM_STATE\n",
    "    )\n",
    "    all_results.append(scenario_results)\n",
    "\n",
    "# Combine all results into master DataFrame\n",
    "master_results = pd.concat(all_results, ignore_index=True)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ All scenarios complete! Total results: {len(master_results)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964b6dd",
   "metadata": {},
   "source": [
    "## 4. Results Output and Export\n",
    "\n",
    "The scenario runner has completed execution. Below we:\n",
    "1. Display the **MASTER results table** with all scenarios and approaches\n",
    "2. Display **per-scenario tables** with English headings\n",
    "3. Export results to:\n",
    "   - `results_master.csv` - All results in flat CSV format\n",
    "   - `results_by_scenario.xlsx` - Excel workbook with one sheet per scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b762b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "MASTER RESULTS TABLE - All Scenarios and Approaches\n",
      "========================================================================================================================\n",
      " Scenario          Approach  CV_AUC_mean  CV_AUC_std  Test_AUC  Test_AP  Optimal_Threshold  Precision_opt  Recall_opt  F1_opt\n",
      "        1      RandomForest       0.9345      0.0006    0.8845   0.4414             0.2560         0.1268      0.8246  0.2198\n",
      "        1           XGBoost       0.9404      0.0003    0.8978   0.4889             0.3293         0.1613      0.7894  0.2679\n",
      "        1          LightGBM       0.9381      0.0008    0.9003   0.5042             0.4406         0.1703      0.7704  0.2789\n",
      "        1          CatBoost       0.9182      0.0008    0.8933   0.4800             0.4826         0.1500      0.7793  0.2515\n",
      "        1 Stacking_Weighted          NaN         NaN    0.9027   0.4994             0.3824         0.1631      0.7889  0.2703\n",
      "        1 Stacking_Logistic          NaN         NaN    0.8732   0.4625             0.2303         0.1394      0.7958  0.2373\n",
      "        1    Stacking_Ridge          NaN         NaN    0.8734   0.4626             0.2309         0.1397      0.7958  0.2376\n",
      "        2      RandomForest       0.9336      0.0004    0.8889   0.4605             0.2977         0.1503      0.7857  0.2523\n",
      "        2           XGBoost       0.9410      0.0001    0.9047   0.5043             0.3420         0.1621      0.8027  0.2697\n",
      "        2          LightGBM       0.9396      0.0015    0.9075   0.5128             0.4031         0.1565      0.8098  0.2623\n",
      "        2          CatBoost       0.9179      0.0015    0.8883   0.4545             0.4534         0.1414      0.7872  0.2397\n",
      "        2 Stacking_Weighted          NaN         NaN    0.9053   0.5080             0.3646         0.1473      0.8095  0.2492\n",
      "        2 Stacking_Logistic          NaN         NaN    0.8936   0.4888             0.2981         0.1501      0.8051  0.2531\n",
      "        2    Stacking_Ridge          NaN         NaN    0.8937   0.4889             0.2989         0.1503      0.8046  0.2533\n",
      "        3      RandomForest       0.9307      0.0008    0.8807   0.4409             0.2674         0.1244      0.8127  0.2158\n",
      "        3           XGBoost       0.9403      0.0008    0.9008   0.4969             0.3117         0.1642      0.7906  0.2719\n",
      "        3          LightGBM       0.9380      0.0014    0.9000   0.5053             0.4139         0.1535      0.7879  0.2569\n",
      "        3          CatBoost       0.9184      0.0010    0.8894   0.4774             0.5221         0.1659      0.7515  0.2718\n",
      "        3 Stacking_Weighted          NaN         NaN    0.9018   0.5022             0.3862         0.1667      0.7815  0.2748\n",
      "        3 Stacking_Logistic          NaN         NaN    0.8734   0.4763             0.3073         0.1794      0.7458  0.2892\n",
      "        3    Stacking_Ridge          NaN         NaN    0.8736   0.4764             0.3072         0.1792      0.7461  0.2890\n",
      "        4      RandomForest       0.9298      0.0011    0.8820   0.4253             0.2909         0.1311      0.8046  0.2255\n",
      "        4           XGBoost       0.9345      0.0004    0.8974   0.4860             0.3569         0.1391      0.8211  0.2379\n",
      "        4          LightGBM       0.9318      0.0008    0.8998   0.4907             0.4309         0.1472      0.8113  0.2492\n",
      "        4          CatBoost       0.9110      0.0009    0.8863   0.4534             0.5328         0.1584      0.7564  0.2619\n",
      "        4 Stacking_Weighted          NaN         NaN    0.8987   0.4814             0.4144         0.1536      0.7965  0.2576\n",
      "        4 Stacking_Logistic          NaN         NaN    0.8836   0.4676             0.3879         0.1646      0.7756  0.2716\n",
      "        4    Stacking_Ridge          NaN         NaN    0.8836   0.4676             0.3881         0.1647      0.7756  0.2717\n",
      "        5      RandomForest       0.9343      0.0004    0.8864   0.4440             0.2554         0.1289      0.8194  0.2228\n",
      "        5           XGBoost       0.9404      0.0009    0.8977   0.4950             0.3426         0.1647      0.7840  0.2722\n",
      "        5          LightGBM       0.9384      0.0012    0.9014   0.5090             0.3841         0.1483      0.8012  0.2503\n",
      "        5          CatBoost       0.9182      0.0010    0.8904   0.4737             0.5275         0.1640      0.7539  0.2694\n",
      "        5 Stacking_Weighted          NaN         NaN    0.9023   0.4994             0.4226         0.1904      0.7591  0.3045\n",
      "        5 Stacking_Logistic          NaN         NaN    0.8754   0.4746             0.2750         0.1544      0.7756  0.2576\n",
      "        5    Stacking_Ridge          NaN         NaN    0.8757   0.4748             0.2764         0.1550      0.7753  0.2583\n",
      "\n",
      "üèÜ Best Overall: Scenario 2 - LightGBM (Test AUC: 0.9075)\n",
      "\n",
      "========================================================================================================================\n",
      "RESULTS BY SCENARIO\n",
      "========================================================================================================================\n",
      "\n",
      "================================================================================\n",
      "Scenario 1 Results: Baseline (Control)\n",
      "(Missing=-999, full 175 features)\n",
      "================================================================================\n",
      "         Approach  CV_AUC_mean  CV_AUC_std  Test_AUC  Test_AP  Optimal_Threshold  Precision_opt  Recall_opt  F1_opt\n",
      "Stacking_Weighted          NaN         NaN    0.9027   0.4994             0.3824         0.1631      0.7889  0.2703\n",
      "         LightGBM       0.9381      0.0008    0.9003   0.5042             0.4406         0.1703      0.7704  0.2789\n",
      "          XGBoost       0.9404      0.0003    0.8978   0.4889             0.3293         0.1613      0.7894  0.2679\n",
      "         CatBoost       0.9182      0.0008    0.8933   0.4800             0.4826         0.1500      0.7793  0.2515\n",
      "     RandomForest       0.9345      0.0006    0.8845   0.4414             0.2560         0.1268      0.8246  0.2198\n",
      "   Stacking_Ridge          NaN         NaN    0.8734   0.4626             0.2309         0.1397      0.7958  0.2376\n",
      "Stacking_Logistic          NaN         NaN    0.8732   0.4625             0.2303         0.1394      0.7958  0.2373\n",
      "\n",
      "  Best: Stacking_Weighted (Test AUC: 0.9027)\n",
      "\n",
      "================================================================================\n",
      "Scenario 2 Results: NaN Strategy\n",
      "(Missing=NaN, full 175 features)\n",
      "================================================================================\n",
      "         Approach  CV_AUC_mean  CV_AUC_std  Test_AUC  Test_AP  Optimal_Threshold  Precision_opt  Recall_opt  F1_opt\n",
      "         LightGBM       0.9396      0.0015    0.9075   0.5128             0.4031         0.1565      0.8098  0.2623\n",
      "Stacking_Weighted          NaN         NaN    0.9053   0.5080             0.3646         0.1473      0.8095  0.2492\n",
      "          XGBoost       0.9410      0.0001    0.9047   0.5043             0.3420         0.1621      0.8027  0.2697\n",
      "   Stacking_Ridge          NaN         NaN    0.8937   0.4889             0.2989         0.1503      0.8046  0.2533\n",
      "Stacking_Logistic          NaN         NaN    0.8936   0.4888             0.2981         0.1501      0.8051  0.2531\n",
      "     RandomForest       0.9336      0.0004    0.8889   0.4605             0.2977         0.1503      0.7857  0.2523\n",
      "         CatBoost       0.9179      0.0015    0.8883   0.4545             0.4534         0.1414      0.7872  0.2397\n",
      "\n",
      "  Best: LightGBM (Test AUC: 0.9075)\n",
      "\n",
      "================================================================================\n",
      "Scenario 3 Results: No Interaction Features\n",
      "(Missing=-999, no inter_* features)\n",
      "================================================================================\n",
      "         Approach  CV_AUC_mean  CV_AUC_std  Test_AUC  Test_AP  Optimal_Threshold  Precision_opt  Recall_opt  F1_opt\n",
      "Stacking_Weighted          NaN         NaN    0.9018   0.5022             0.3862         0.1667      0.7815  0.2748\n",
      "          XGBoost       0.9403      0.0008    0.9008   0.4969             0.3117         0.1642      0.7906  0.2719\n",
      "         LightGBM       0.9380      0.0014    0.9000   0.5053             0.4139         0.1535      0.7879  0.2569\n",
      "         CatBoost       0.9184      0.0010    0.8894   0.4774             0.5221         0.1659      0.7515  0.2718\n",
      "     RandomForest       0.9307      0.0008    0.8807   0.4409             0.2674         0.1244      0.8127  0.2158\n",
      "   Stacking_Ridge          NaN         NaN    0.8736   0.4764             0.3072         0.1792      0.7461  0.2890\n",
      "Stacking_Logistic          NaN         NaN    0.8734   0.4763             0.3073         0.1794      0.7458  0.2892\n",
      "\n",
      "  Best: Stacking_Weighted (Test AUC: 0.9018)\n",
      "\n",
      "================================================================================\n",
      "Scenario 4 Results: Strong Features Only\n",
      "(Missing=-999, strong_features + strong_cat)\n",
      "================================================================================\n",
      "         Approach  CV_AUC_mean  CV_AUC_std  Test_AUC  Test_AP  Optimal_Threshold  Precision_opt  Recall_opt  F1_opt\n",
      "         LightGBM       0.9318      0.0008    0.8998   0.4907             0.4309         0.1472      0.8113  0.2492\n",
      "Stacking_Weighted          NaN         NaN    0.8987   0.4814             0.4144         0.1536      0.7965  0.2576\n",
      "          XGBoost       0.9345      0.0004    0.8974   0.4860             0.3569         0.1391      0.8211  0.2379\n",
      "         CatBoost       0.9110      0.0009    0.8863   0.4534             0.5328         0.1584      0.7564  0.2619\n",
      "Stacking_Logistic          NaN         NaN    0.8836   0.4676             0.3879         0.1646      0.7756  0.2716\n",
      "   Stacking_Ridge          NaN         NaN    0.8836   0.4676             0.3881         0.1647      0.7756  0.2717\n",
      "     RandomForest       0.9298      0.0011    0.8820   0.4253             0.2909         0.1311      0.8046  0.2255\n",
      "\n",
      "  Best: LightGBM (Test AUC: 0.8998)\n",
      "\n",
      "================================================================================\n",
      "Scenario 5 Results: Strong + Moderate Features\n",
      "(Missing=-999, strong + moderate features)\n",
      "================================================================================\n",
      "         Approach  CV_AUC_mean  CV_AUC_std  Test_AUC  Test_AP  Optimal_Threshold  Precision_opt  Recall_opt  F1_opt\n",
      "Stacking_Weighted          NaN         NaN    0.9023   0.4994             0.4226         0.1904      0.7591  0.3045\n",
      "         LightGBM       0.9384      0.0012    0.9014   0.5090             0.3841         0.1483      0.8012  0.2503\n",
      "          XGBoost       0.9404      0.0009    0.8977   0.4950             0.3426         0.1647      0.7840  0.2722\n",
      "         CatBoost       0.9182      0.0010    0.8904   0.4737             0.5275         0.1640      0.7539  0.2694\n",
      "     RandomForest       0.9343      0.0004    0.8864   0.4440             0.2554         0.1289      0.8194  0.2228\n",
      "   Stacking_Ridge          NaN         NaN    0.8757   0.4748             0.2764         0.1550      0.7753  0.2583\n",
      "Stacking_Logistic          NaN         NaN    0.8754   0.4746             0.2750         0.1544      0.7756  0.2576\n",
      "\n",
      "  Best: Stacking_Weighted (Test AUC: 0.9023)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Format and Display Results\n",
    "# =============================================================================\n",
    "\n",
    "# Round numeric columns for display\n",
    "numeric_cols = ['CV_AUC_mean', 'CV_AUC_std', 'Test_AUC', 'Test_AP', \n",
    "                'Optimal_Threshold', 'Precision_opt', 'Recall_opt', 'F1_opt']\n",
    "\n",
    "results_display = master_results.copy()\n",
    "for col in numeric_cols:\n",
    "    results_display[col] = results_display[col].round(4)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. MASTER TABLE (All Scenarios √ó All Approaches)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"MASTER RESULTS TABLE - All Scenarios and Approaches\")\n",
    "print(\"=\"*120)\n",
    "print(results_display.to_string(index=False))\n",
    "\n",
    "# Best overall approach\n",
    "best_row = results_display.loc[results_display['Test_AUC'].idxmax()]\n",
    "print(f\"\\nüèÜ Best Overall: Scenario {int(best_row['Scenario'])} - {best_row['Approach']} \"\n",
    "      f\"(Test AUC: {best_row['Test_AUC']:.4f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. PER-SCENARIO TABLES\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"RESULTS BY SCENARIO\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "scenario_results_dict = {}\n",
    "\n",
    "for scenario_id in sorted(master_results['Scenario'].unique()):\n",
    "    scenario_name = SCENARIOS[scenario_id]['name']\n",
    "    scenario_desc = SCENARIOS[scenario_id]['description']\n",
    "    \n",
    "    scenario_df = results_display[results_display['Scenario'] == scenario_id].copy()\n",
    "    scenario_df = scenario_df.drop(columns=['Scenario'])  # Remove redundant column\n",
    "    scenario_df = scenario_df.sort_values('Test_AUC', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    scenario_results_dict[scenario_id] = scenario_df\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Scenario {scenario_id} Results: {scenario_name}\")\n",
    "    print(f\"({scenario_desc})\")\n",
    "    print(\"=\"*80)\n",
    "    print(scenario_df.to_string(index=False))\n",
    "    \n",
    "    # Best in scenario\n",
    "    best_in_scenario = scenario_df.iloc[0]\n",
    "    print(f\"\\n  Best: {best_in_scenario['Approach']} (Test AUC: {best_in_scenario['Test_AUC']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1cdcf3",
   "metadata": {},
   "source": [
    "## 5. Export Results to Files\n",
    "\n",
    "Export results in two formats:\n",
    "1. **CSV**: Flat file with all results (`results_master.csv`)\n",
    "2. **Excel**: Multi-sheet workbook with one sheet per scenario (`results_by_scenario.xlsx`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b5b8a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: c:\\Users\\Abdulkadir\\Desktop\\Uygulama √ßalƒ±≈ümalarƒ±\\Fraud_Detection\\Fraud_Detection\\results\\results_master.csv\n",
      "‚úÖ Saved: c:\\Users\\Abdulkadir\\Desktop\\Uygulama √ßalƒ±≈ümalarƒ±\\Fraud_Detection\\Fraud_Detection\\results\\results_by_scenario.xlsx\n",
      "\n",
      "üìÅ Output files saved to: c:\\Users\\Abdulkadir\\Desktop\\Uygulama √ßalƒ±≈ümalarƒ±\\Fraud_Detection\\Fraud_Detection\\results\n",
      "   - results_master.csv (all results)\n",
      "   - results_by_scenario.xlsx (Excel with 5 scenario sheets + Master)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Export Results to Files\n",
    "# =============================================================================\n",
    "\n",
    "# Create results directory if needed\n",
    "results_dir = ROOT / \"results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Save Master CSV\n",
    "# =============================================================================\n",
    "csv_path = results_dir / \"results_master.csv\"\n",
    "results_display.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ Saved: {csv_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Save Excel with Per-Scenario Sheets\n",
    "# =============================================================================\n",
    "excel_path = results_dir / \"results_by_scenario.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    # Master sheet\n",
    "    results_display.to_excel(writer, sheet_name='Master', index=False)\n",
    "    \n",
    "    # Per-scenario sheets\n",
    "    for scenario_id, scenario_df in scenario_results_dict.items():\n",
    "        sheet_name = f\"Scenario_{scenario_id}\"\n",
    "        scenario_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"‚úÖ Saved: {excel_path}\")\n",
    "print(f\"\\nüìÅ Output files saved to: {results_dir}\")\n",
    "print(f\"   - results_master.csv (all results)\")\n",
    "print(f\"   - results_by_scenario.xlsx (Excel with {len(SCENARIOS)} scenario sheets + Master)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46c95d",
   "metadata": {},
   "source": [
    "## 6. Summary Analysis\n",
    "\n",
    "Quick comparison of scenario performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04c2bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Best Approach per Scenario (by Test AUC):\n",
      "------------------------------------------------------------\n",
      "  Scenario 1 (Baseline (Control)): Stacking_Weighted (AUC=0.9027)\n",
      "  Scenario 2 (NaN Strategy): LightGBM (AUC=0.9075)\n",
      "  Scenario 3 (No Interaction Features): Stacking_Weighted (AUC=0.9018)\n",
      "  Scenario 4 (Strong Features Only): LightGBM (AUC=0.8998)\n",
      "  Scenario 5 (Strong + Moderate Features): Stacking_Weighted (AUC=0.9023)\n",
      "\n",
      "üèÜ Best Overall Scenario: 2 - NaN Strategy\n",
      "   Best Approach: LightGBM\n",
      "   Test AUC: 0.9075\n",
      "   Test AP (PR-AUC): 0.5128\n",
      "\n",
      "üìà Stacking vs Base Models Comparison:\n",
      "------------------------------------------------------------\n",
      "  Scenario 1: Best Base=0.9003, Best Stack=0.9027, Œî=+0.0024 ‚úÖ\n",
      "  Scenario 2: Best Base=0.9075, Best Stack=0.9053, Œî=-0.0022 ‚ö†Ô∏è\n",
      "  Scenario 3: Best Base=0.9008, Best Stack=0.9018, Œî=+0.0010 ‚úÖ\n",
      "  Scenario 4: Best Base=0.8998, Best Stack=0.8987, Œî=-0.0010 ‚ö†Ô∏è\n",
      "  Scenario 5: Best Base=0.9014, Best Stack=0.9023, Œî=+0.0009 ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Summary Analysis: Best Approaches by Scenario\n",
    "# =============================================================================\n",
    "\n",
    "# Best approach per scenario\n",
    "print(\"üìä Best Approach per Scenario (by Test AUC):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "summary_rows = []\n",
    "for scenario_id in sorted(SCENARIOS.keys()):\n",
    "    scenario_df = master_results[master_results['Scenario'] == scenario_id]\n",
    "    best_row = scenario_df.loc[scenario_df['Test_AUC'].idxmax()]\n",
    "    \n",
    "    summary_rows.append({\n",
    "        'Scenario': scenario_id,\n",
    "        'Name': SCENARIOS[scenario_id]['name'],\n",
    "        'Features': len(SCENARIOS[scenario_id]['features']),\n",
    "        'Best_Approach': best_row['Approach'],\n",
    "        'Test_AUC': best_row['Test_AUC'],\n",
    "        'Test_AP': best_row['Test_AP']\n",
    "    })\n",
    "    \n",
    "    print(f\"  Scenario {scenario_id} ({SCENARIOS[scenario_id]['name']}): \"\n",
    "          f\"{best_row['Approach']} (AUC={best_row['Test_AUC']:.4f})\")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Best overall scenario\n",
    "best_scenario_row = summary_df.loc[summary_df['Test_AUC'].idxmax()]\n",
    "print(f\"\\nüèÜ Best Overall Scenario: {int(best_scenario_row['Scenario'])} - {best_scenario_row['Name']}\")\n",
    "print(f\"   Best Approach: {best_scenario_row['Best_Approach']}\")\n",
    "print(f\"   Test AUC: {best_scenario_row['Test_AUC']:.4f}\")\n",
    "print(f\"   Test AP (PR-AUC): {best_scenario_row['Test_AP']:.4f}\")\n",
    "\n",
    "# Comparison: Base Models vs Stacking\n",
    "print(\"\\nüìà Stacking vs Base Models Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "base_approaches = ['RandomForest', 'XGBoost', 'LightGBM', 'CatBoost']\n",
    "stacking_approaches = ['Stacking_Weighted', 'Stacking_Logistic', 'Stacking_Ridge']\n",
    "\n",
    "for scenario_id in sorted(SCENARIOS.keys()):\n",
    "    scenario_df = master_results[master_results['Scenario'] == scenario_id]\n",
    "    \n",
    "    best_base_auc = scenario_df[scenario_df['Approach'].isin(base_approaches)]['Test_AUC'].max()\n",
    "    best_stack_auc = scenario_df[scenario_df['Approach'].isin(stacking_approaches)]['Test_AUC'].max()\n",
    "    \n",
    "    improvement = best_stack_auc - best_base_auc\n",
    "    status = \"‚úÖ\" if improvement > 0 else \"‚ö†Ô∏è\"\n",
    "    \n",
    "    print(f\"  Scenario {scenario_id}: Best Base={best_base_auc:.4f}, Best Stack={best_stack_auc:.4f}, \"\n",
    "          f\"Œî={improvement:+.4f} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b424f55",
   "metadata": {},
   "source": [
    "## 7. Execution Complete\n",
    "\n",
    "All 5 scenarios have been evaluated. Results are saved to:\n",
    "- `results/results_master.csv`\n",
    "- `results/results_by_scenario.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ff9fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "FINAL SUMMARY - Scenario Comparison\n",
      "========================================================================================================================\n",
      " Scenario                       Name  N_Features     Best_Approach  Test_AUC  Test_AP\n",
      "        1         Baseline (Control)         175 Stacking_Weighted    0.9027   0.4994\n",
      "        2               NaN Strategy         175          LightGBM    0.9075   0.5128\n",
      "        3    No Interaction Features         145 Stacking_Weighted    0.9018   0.5022\n",
      "        4       Strong Features Only         118          LightGBM    0.8998   0.4907\n",
      "        5 Strong + Moderate Features         175 Stacking_Weighted    0.9023   0.4994\n",
      "\n",
      "‚úÖ Notebook execution complete!\n",
      "   Total scenarios: 5\n",
      "   Approaches per scenario: 7 (4 base + 3 stacking)\n",
      "   Total results: 35 rows\n",
      "\n",
      "üìÅ Files saved:\n",
      "   - c:\\Users\\Abdulkadir\\Desktop\\Uygulama √ßalƒ±≈ümalarƒ±\\Fraud_Detection\\Fraud_Detection\\results\\results_master.csv\n",
      "   - c:\\Users\\Abdulkadir\\Desktop\\Uygulama √ßalƒ±≈ümalarƒ±\\Fraud_Detection\\Fraud_Detection\\results\\results_by_scenario.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Final Summary Table\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"FINAL SUMMARY - Scenario Comparison\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Create summary comparison table\n",
    "final_summary = summary_df[['Scenario', 'Name', 'Features', 'Best_Approach', 'Test_AUC', 'Test_AP']].copy()\n",
    "final_summary.columns = ['Scenario', 'Name', 'N_Features', 'Best_Approach', 'Test_AUC', 'Test_AP']\n",
    "final_summary['Test_AUC'] = final_summary['Test_AUC'].round(4)\n",
    "final_summary['Test_AP'] = final_summary['Test_AP'].round(4)\n",
    "\n",
    "print(final_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ Notebook execution complete!\")\n",
    "print(f\"   Total scenarios: {len(SCENARIOS)}\")\n",
    "print(f\"   Approaches per scenario: 7 (4 base + 3 stacking)\")\n",
    "print(f\"   Total results: {len(master_results)} rows\")\n",
    "print(f\"\\nüìÅ Files saved:\")\n",
    "print(f\"   - {results_dir / 'results_master.csv'}\")\n",
    "print(f\"   - {results_dir / 'results_by_scenario.xlsx'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
