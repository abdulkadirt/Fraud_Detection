{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64459b7",
   "metadata": {},
   "source": [
    "# Fraud Detection - Model Training & Evaluation\n",
    "\n",
    "This notebook focuses on training and evaluating machine learning models for the IEEE-CIS Fraud Detection task.\n",
    "\n",
    "## Why Tree-Based Models?\n",
    "\n",
    "Tree-based ensemble methods are particularly well-suited for fraud detection due to:\n",
    "\n",
    "1. **Robustness to Feature Distributions**: No assumptions about normality; handles skewed transaction amounts naturally\n",
    "2. **Missing Value Handling**: Native support for missing values (especially LightGBM, XGBoost)\n",
    "3. **Feature Interactions**: Automatically captures non-linear relationships and complex interactions\n",
    "4. **Imbalanced Data**: Built-in mechanisms (`scale_pos_weight`, `is_unbalance`) for class imbalance\n",
    "5. **Correlation Insensitivity**: Unlike linear models, not affected by multicollinearity\n",
    "6. **Outlier Robustness**: Split-based decisions are less sensitive to extreme values\n",
    "7. **Mixed Feature Types**: Handles both numerical and encoded categorical features seamlessly\n",
    "\n",
    "**Models to Compare**:\n",
    "1. Random Forest - Bagging ensemble, robust baseline\n",
    "2. XGBoost - Gradient boosting with regularization\n",
    "3. LightGBM - Fast gradient boosting, leaf-wise growth\n",
    "4. CatBoost - Native categorical handling, ordered boosting\n",
    "5. **Stacking Ensemble** - Combines base models for improved performance\n",
    "\n",
    "**Evaluation Strategy**: \n",
    "- Stratified 5-Fold Cross-Validation for OOF predictions\n",
    "- Stacking uses OOF predictions to avoid data leakage\n",
    "- Primary metric: ROC-AUC (standard for fraud detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96aaa5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Import Libraries\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Boosting models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Custom functions\n",
    "import sys\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT / \"functions\"))\n",
    "\n",
    "from model_functions import (\n",
    "    compare_roc_curves, compare_models, plot_feature_importance,\n",
    "    full_evaluation_report, find_optimal_threshold\n",
    ")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0c973",
   "metadata": {},
   "source": [
    "## 1. Load Data & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "547fd62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded successfully!\n",
      "   Train shape: (472432, 195)\n",
      "   Test shape: (118108, 195)\n",
      "   Features for modeling: 175\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Preprocessed Data from EDA.ipynb\n",
    "# =============================================================================\n",
    "import pickle\n",
    "\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "# Load DataFrames\n",
    "train_df = pd.read_parquet(DATA / \"train_preprocessed.parquet\")\n",
    "test_df = pd.read_parquet(DATA / \"test_preprocessed.parquet\")\n",
    "\n",
    "# Load feature lists\n",
    "with open(DATA / \"feature_lists.pkl\", 'rb') as f:\n",
    "    feature_lists = pickle.load(f)\n",
    "\n",
    "filtered_features = feature_lists['filtered_features']\n",
    "categorical_features = feature_lists['categorical_for_model']\n",
    "strong_features = feature_lists['strong_features']\n",
    "moderate_features = feature_lists['moderate_features']\n",
    "\n",
    "print(f\"âœ… Data loaded successfully!\")\n",
    "print(f\"   Train shape: {train_df.shape}\")\n",
    "print(f\"   Test shape: {test_df.shape}\")\n",
    "print(f\"   Features for modeling: {len(filtered_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54a80d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data prepared: X_train (472432, 175), X_test (118108, 175)\n",
      "   Train fraud rate: 3.51%\n",
      "   Test fraud rate: 3.44%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Prepare Features and Target\n",
    "# =============================================================================\n",
    "X_train = train_df[filtered_features].copy()\n",
    "y_train = train_df['isFraud'].copy()\n",
    "X_test = test_df[filtered_features].copy()\n",
    "y_test = test_df['isFraud'].copy()\n",
    "\n",
    "# =============================================================================\n",
    "# Handle Infinity and Missing Values\n",
    "# =============================================================================\n",
    "inf_cols = X_train.columns[X_train.isin([np.inf, -np.inf]).any()].tolist()\n",
    "if inf_cols:\n",
    "    print(f\" Found infinity in {len(inf_cols)} columns: {inf_cols}\")\n",
    "\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "MISSING_FLAG = -999  # yarÄ±ÅŸma birincisinin uygulamasÄ±nda -999 kullanÄ±lmÄ±ÅŸ bÃ¶ylece aÄŸaÃ§ algoritmalarÄ±nda bu kayÄ±tlar direkt ayrÄ±lÄ±yor\n",
    "X_train = X_train.fillna(MISSING_FLAG)\n",
    "X_test = X_test.fillna(MISSING_FLAG)\n",
    "\n",
    "print(f\"   Data prepared: X_train {X_train.shape}, X_test {X_test.shape}\")\n",
    "print(f\"   Train fraud rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"   Test fraud rate: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b72aa",
   "metadata": {},
   "source": [
    "### Data Split Visualization (Temporal Split)\n",
    "\n",
    "**Important Note on Temporal Splitting:**\n",
    "\n",
    "In fraud detection, using a **temporal (time-based) split** is crucial because:\n",
    "- Fraudsters continuously adapt their strategies over time\n",
    "- Models trained on past data must predict future fraud patterns\n",
    "- Random splits would cause **data leakage** from future to past\n",
    "\n",
    "**Implication**: CV scores (computed on training data) may differ from test set performance because the test set represents a **future time period** with potentially different fraud patterns. This is expected behavior and reflects real-world deployment conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbba86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # Visualize Train/Test Split (Temporal)\n",
    "# # =============================================================================\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# # 1. Dataset Size Comparison\n",
    "# ax1 = axes[0]\n",
    "# sizes = [len(X_train), len(X_test)]\n",
    "# labels = ['Train (80%)', 'Test (20%)']\n",
    "# colors = ['#3498db', '#e74c3c']\n",
    "# bars = ax1.bar(labels, sizes, color=colors, edgecolor='black', linewidth=1.2)\n",
    "# ax1.set_ylabel('Number of Samples', fontsize=11)\n",
    "# ax1.set_title('Dataset Split', fontsize=12, fontweight='bold')\n",
    "# for bar, size in zip(bars, sizes):\n",
    "#     ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
    "#              f'{size:,}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# # 2. Fraud Rate Comparison\n",
    "# ax2 = axes[1]\n",
    "# fraud_rates = [y_train.mean()*100, y_test.mean()*100]\n",
    "# bars2 = ax2.bar(labels, fraud_rates, color=colors, edgecolor='black', linewidth=1.2)\n",
    "# ax2.set_ylabel('Fraud Rate (%)', fontsize=11)\n",
    "# ax2.set_title('Fraud Rate by Split', fontsize=12, fontweight='bold')\n",
    "# ax2.axhline(y=y_train.mean()*100, color='gray', linestyle='--', alpha=0.5, label='Train Rate')\n",
    "# for bar, rate in zip(bars2, fraud_rates):\n",
    "#     ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "#              f'{rate:.2f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# # 3. Class Distribution (Stacked)\n",
    "# ax3 = axes[2]\n",
    "# train_counts = [len(y_train) - y_train.sum(), y_train.sum()]\n",
    "# test_counts = [len(y_test) - y_test.sum(), y_test.sum()]\n",
    "# x_pos = np.arange(2)\n",
    "# width = 0.35\n",
    "\n",
    "# bars_normal = ax3.bar(x_pos - width/2, [train_counts[0], test_counts[0]], width, \n",
    "#                        label='Normal', color='#2ecc71', edgecolor='black')\n",
    "# bars_fraud = ax3.bar(x_pos + width/2, [train_counts[1], test_counts[1]], width, \n",
    "#                       label='Fraud', color='#e74c3c', edgecolor='black')\n",
    "# ax3.set_xticks(x_pos)\n",
    "# ax3.set_xticklabels(['Train', 'Test'])\n",
    "# ax3.set_ylabel('Count', fontsize=11)\n",
    "# ax3.set_title('Class Distribution', fontsize=12, fontweight='bold')\n",
    "# ax3.legend(loc='upper right')\n",
    "# ax3.set_yscale('log')  # Log scale due to imbalance\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Summary table\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"DATA SPLIT SUMMARY (Temporal Split: Train=Past, Test=Future)\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"{'Dataset':<12} {'Samples':>12} {'Normal':>12} {'Fraud':>10} {'Fraud %':>10}\")\n",
    "# print(\"-\"*60)\n",
    "# print(f\"{'Train':<12} {len(X_train):>12,} {int(len(y_train)-y_train.sum()):>12,} {int(y_train.sum()):>10,} {y_train.mean()*100:>9.2f}%\")\n",
    "# print(f\"{'Test':<12} {len(X_test):>12,} {int(len(y_test)-y_test.sum()):>12,} {int(y_test.sum()):>10,} {y_test.mean()*100:>9.2f}%\")\n",
    "# print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c445f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "455e89c0",
   "metadata": {},
   "source": [
    "## 2. Model Definitions & Stacking Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "407bf2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Configuration:\n",
      "   SMOTE Method: None (using class weights)\n",
      "   Stacking Method: weighted\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Change these variables to control the pipeline\n",
    "# =============================================================================\n",
    "\n",
    "# SMOTE Configuration:\n",
    "#   None          - No resampling (use built-in class weights)\n",
    "#   'smote'       - Standard SMOTE oversampling\n",
    "#   'smote_tomek' - SMOTE + Tomek links (cleaner decision boundaries)\n",
    "\n",
    "SMOTE_METHOD = None  # <-- CHANGE THIS TO SWITCH SMOTE METHOD\n",
    "\n",
    "# Stacking Configuration:\n",
    "#   'logistic'  - Logistic Regression as meta-model (recommended for probability calibration)\n",
    "#   'ridge'     - Ridge Classifier as meta-model (more regularization)\n",
    "#   'weighted'  - No meta-model, weighted average based on CV performance\n",
    "\n",
    "STACKING_METHOD = 'weighted'  # <-- CHANGE THIS TO SWITCH STACKING METHOD\n",
    "\n",
    "print(f\"ðŸ“‹ Configuration:\")\n",
    "print(f\"   SMOTE Method: {SMOTE_METHOD if SMOTE_METHOD else 'None (using class weights)'}\")\n",
    "print(f\"   Stacking Method: {STACKING_METHOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e38f0d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ Using class weights (imbalance ratio: 27.46)\n",
      "âœ… 4 base models defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Model Definitions (automatically adapts to SMOTE setting)\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate class imbalance ratio (only used when SMOTE is disabled)\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "if SMOTE_METHOD is None:\n",
    "    # Use built-in class weights when SMOTE is disabled\n",
    "    print(f\"âš–ï¸ Using class weights (imbalance ratio: {scale_pos_weight:.2f})\")\n",
    "    \n",
    "    BASE_MODELS = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            eval_metric='auc',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'LightGBM': LGBMClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            is_unbalance=True,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ),\n",
    "        'CatBoost': CatBoostClassifier(\n",
    "            iterations=300,\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            auto_class_weights='Balanced',\n",
    "            eval_metric='AUC',\n",
    "            random_seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    }\n",
    "else:\n",
    "    # No class weights needed when SMOTE balances the data\n",
    "    print(f\"âš–ï¸ SMOTE active - class weights disabled (data is balanced)\")\n",
    "    \n",
    "    BASE_MODELS = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            eval_metric='auc',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'LightGBM': LGBMClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ),\n",
    "        'CatBoost': CatBoostClassifier(\n",
    "            iterations=300,\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            eval_metric='AUC',\n",
    "            random_seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    }\n",
    "\n",
    "print(f\"âœ… {len(BASE_MODELS)} base models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae8680",
   "metadata": {},
   "source": [
    "### Apply SMOTE Resampling (Configured Above)\n",
    "\n",
    "The `SMOTE_METHOD` variable controls how class imbalance is handled:\n",
    "\n",
    "| Setting | Description | When to Use |\n",
    "|---------|-------------|-------------|\n",
    "| `None` | No resampling, use built-in class weights | Default, usually works well |\n",
    "| `'smote'` | Synthetic Minority Over-sampling | When class weights underperform |\n",
    "| `'smote_tomek'` | SMOTE + Tomek links cleanup | For cleaner decision boundaries |\n",
    "\n",
    "**Note:** When SMOTE is active, model class weights are automatically disabled since the data is already balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "114c1664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ SMOTE disabled - using built-in class weights for imbalance handling\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Apply SMOTE (if configured)\n",
    "# =============================================================================\n",
    "\n",
    "# Store original data for reference\n",
    "X_train_original = X_train.copy()\n",
    "y_train_original = y_train.copy()\n",
    "\n",
    "if SMOTE_METHOD is not None:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    \n",
    "    print(f\"Applying {SMOTE_METHOD.upper()} to training data...\")\n",
    "    print(f\"Before: {len(X_train):,} samples (Fraud: {y_train.sum():,}, Rate: {y_train.mean()*100:.2f}%)\")\n",
    "    \n",
    "    if SMOTE_METHOD == 'smote':\n",
    "        sampler = SMOTE(\n",
    "            sampling_strategy=0.5,  # Ratio of minority to majority\n",
    "            random_state=42,\n",
    "            k_neighbors=5,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    elif SMOTE_METHOD == 'smote_tomek':\n",
    "        sampler = SMOTETomek(\n",
    "            sampling_strategy=0.5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown SMOTE method: {SMOTE_METHOD}. Use None, 'smote', or 'smote_tomek'\")\n",
    "    \n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"After:  {len(X_train):,} samples (Fraud: {y_train.sum():,}, Rate: {y_train.mean()*100:.2f}%)\")\n",
    "    \n",
    "    # Visualize the resampling effect\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Before SMOTE\n",
    "    ax1 = axes[0]\n",
    "    counts_before = [len(y_train_original) - y_train_original.sum(), y_train_original.sum()]\n",
    "    ax1.bar(['Normal', 'Fraud'], counts_before, color=['#2ecc71', '#e74c3c'], edgecolor='black')\n",
    "    ax1.set_title('Before SMOTE', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Count')\n",
    "    for i, v in enumerate(counts_before):\n",
    "        ax1.text(i, v + 500, f'{int(v):,}', ha='center', fontweight='bold')\n",
    "    \n",
    "    # After SMOTE\n",
    "    ax2 = axes[1]\n",
    "    counts_after = [len(y_train) - y_train.sum(), y_train.sum()]\n",
    "    ax2.bar(['Normal', 'Fraud'], counts_after, color=['#2ecc71', '#e74c3c'], edgecolor='black')\n",
    "    ax2.set_title(f'After {SMOTE_METHOD.upper()}', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Count')\n",
    "    for i, v in enumerate(counts_after):\n",
    "        ax2.text(i, v + 500, f'{int(v):,}', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš™ï¸ SMOTE disabled - using built-in class weights for imbalance handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5535b5f",
   "metadata": {},
   "source": [
    "## 3. Train All Models with OOF Predictions\n",
    "\n",
    "Train all base models using Stratified K-Fold CV to generate out-of-fold (OOF) predictions.\n",
    "These OOF predictions are used for:\n",
    "1. Evaluating individual model performance\n",
    "2. Training the stacking meta-model (without data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a4fa43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 4 models with 3-Fold CV...\n",
      "SMOTE: Disabled\n",
      "============================================================\n",
      "\n",
      "â–¶ Training RandomForest...\n",
      "   CV AUC: 0.9345 Â± 0.0006\n",
      "   Training time: 140.09 seconds\n",
      "\n",
      "â–¶ Training XGBoost...\n",
      "   CV AUC: 0.9404 Â± 0.0003\n",
      "   Training time: 355.80 seconds\n",
      "\n",
      "â–¶ Training LightGBM...\n",
      "   CV AUC: 0.9381 Â± 0.0008\n",
      "   Training time: 32.24 seconds\n",
      "\n",
      "â–¶ Training CatBoost...\n",
      "   CV AUC: 0.9182 Â± 0.0008\n",
      "   Training time: 121.14 seconds\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Train All Base Models with OOF Predictions\n",
    "# =============================================================================\n",
    "N_FOLDS = 3\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Always use original data size for OOF predictions\n",
    "oof_predictions = {name: np.zeros(len(y_train_original)) for name in BASE_MODELS}\n",
    "test_predictions = {name: np.zeros(len(y_test)) for name in BASE_MODELS}\n",
    "cv_scores = {name: [] for name in BASE_MODELS}\n",
    "trained_models = {name: [] for name in BASE_MODELS}  # Store fold models\n",
    "\n",
    "# Prepare SMOTE sampler if needed (applied per fold to avoid data leakage)\n",
    "if SMOTE_METHOD is not None:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    \n",
    "    if SMOTE_METHOD == 'smote':\n",
    "        sampler = SMOTE(sampling_strategy=0.5, random_state=42, k_neighbors=5, n_jobs=-1)\n",
    "    else:  # smote_tomek\n",
    "        sampler = SMOTETomek(sampling_strategy=0.5, random_state=42, n_jobs=-1)\n",
    "\n",
    "print(f\"Training {len(BASE_MODELS)} models with {N_FOLDS}-Fold CV...\")\n",
    "print(f\"SMOTE: {'Applied per fold' if SMOTE_METHOD else 'Disabled'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for model_name, model in BASE_MODELS.items():\n",
    "    print(f\"\\nâ–¶ Training {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use original data for CV splits (important for fair evaluation)\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_original, y_train_original), 1):\n",
    "        X_tr, X_val = X_train_original.iloc[train_idx], X_train_original.iloc[val_idx]\n",
    "        y_tr, y_val = y_train_original.iloc[train_idx], y_train_original.iloc[val_idx]\n",
    "        \n",
    "        # Apply SMOTE only to training fold (never to validation!)\n",
    "        if SMOTE_METHOD is not None:\n",
    "            X_tr, y_tr = sampler.fit_resample(X_tr, y_tr)\n",
    "        \n",
    "        # Clone and train model\n",
    "        model_clone = model.__class__(**model.get_params())\n",
    "        model_clone.fit(X_tr, y_tr)\n",
    "        \n",
    "        # OOF predictions (on original validation data)\n",
    "        val_probs = model_clone.predict_proba(X_val)[:, 1]\n",
    "        oof_predictions[model_name][val_idx] = val_probs\n",
    "        \n",
    "        # Test predictions (average across folds)\n",
    "        test_predictions[model_name] += model_clone.predict_proba(X_test)[:, 1] / N_FOLDS\n",
    "        \n",
    "        # Store fold score\n",
    "        fold_auc = roc_auc_score(y_val, val_probs)\n",
    "        cv_scores[model_name].append(fold_auc)\n",
    "        \n",
    "        # Store trained model (for feature importance later)\n",
    "        trained_models[model_name].append(model_clone)\n",
    "    \n",
    "    # Calculate training time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Print CV results\n",
    "    mean_auc = np.mean(cv_scores[model_name])\n",
    "    std_auc = np.std(cv_scores[model_name])\n",
    "    print(f\"   CV AUC: {mean_auc:.4f} Â± {std_auc:.4f}\")\n",
    "    print(f\"   Training time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964b6dd",
   "metadata": {},
   "source": [
    "## 4. Stacking Ensemble\n",
    "\n",
    "Combine base model predictions using the configured stacking method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b762b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-features shape: Train (472432, 4), Test (118108, 4)\n",
      "Stacking method: weighted\n",
      "Weights based on CV AUC: {'RandomForest': 0.25, 'XGBoost': 0.252, 'LightGBM': 0.251, 'CatBoost': 0.246}\n",
      "\n",
      "âœ… Stacking ensemble created!\n",
      "   Stacking OOF AUC: 0.9375\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Build Stacking Ensemble\n",
    "# =============================================================================\n",
    "\n",
    "# Create meta-features from OOF predictions\n",
    "meta_train = np.column_stack([oof_predictions[name] for name in BASE_MODELS])\n",
    "meta_test = np.column_stack([test_predictions[name] for name in BASE_MODELS])\n",
    "\n",
    "print(f\"Meta-features shape: Train {meta_train.shape}, Test {meta_test.shape}\")\n",
    "print(f\"Stacking method: {STACKING_METHOD}\")\n",
    "\n",
    "if STACKING_METHOD == 'logistic':\n",
    "    # Logistic Regression meta-model\n",
    "    meta_model = LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Scale meta-features for logistic regression\n",
    "    scaler = StandardScaler()\n",
    "    meta_train_scaled = scaler.fit_transform(meta_train)\n",
    "    meta_test_scaled = scaler.transform(meta_test)\n",
    "    \n",
    "    meta_model.fit(meta_train_scaled, y_train_original)\n",
    "    stacking_probs = meta_model.predict_proba(meta_test_scaled)[:, 1]\n",
    "    stacking_oof = meta_model.predict_proba(scaler.transform(meta_train))[:, 1]\n",
    "    \n",
    "    print(f\"Meta-model coefficients: {dict(zip(BASE_MODELS.keys(), meta_model.coef_[0].round(3)))}\")\n",
    "\n",
    "elif STACKING_METHOD == 'ridge':\n",
    "    # Ridge Classifier meta-model\n",
    "    meta_model = RidgeClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    scaler = StandardScaler()\n",
    "    meta_train_scaled = scaler.fit_transform(meta_train)\n",
    "    meta_test_scaled = scaler.transform(meta_test)\n",
    "    \n",
    "    meta_model.fit(meta_train_scaled, y_train_original)\n",
    "    # Ridge doesn't have predict_proba, use decision_function\n",
    "    stacking_probs = meta_model.decision_function(meta_test_scaled)\n",
    "    stacking_oof = meta_model.decision_function(scaler.transform(meta_train))\n",
    "    # Normalize to [0, 1] range for comparison\n",
    "    stacking_probs = (stacking_probs - stacking_probs.min()) / (stacking_probs.max() - stacking_probs.min())\n",
    "    stacking_oof = (stacking_oof - stacking_oof.min()) / (stacking_oof.max() - stacking_oof.min())\n",
    "    \n",
    "    print(f\"Meta-model coefficients: {dict(zip(BASE_MODELS.keys(), meta_model.coef_[0].round(3)))}\")\n",
    "\n",
    "elif STACKING_METHOD == 'weighted':\n",
    "    # Weighted average based on CV performance\n",
    "    cv_aucs = {name: np.mean(cv_scores[name]) for name in BASE_MODELS}\n",
    "    total_auc = sum(cv_aucs.values())\n",
    "    weights = {name: auc / total_auc for name, auc in cv_aucs.items()}\n",
    "    \n",
    "    print(f\"Weights based on CV AUC: {dict((k, round(v, 3)) for k, v in weights.items())}\")\n",
    "    \n",
    "    stacking_probs = np.zeros(len(y_test))\n",
    "    stacking_oof = np.zeros(len(y_train_original))\n",
    "    for name in BASE_MODELS:\n",
    "        stacking_probs += test_predictions[name] * weights[name]\n",
    "        stacking_oof += oof_predictions[name] * weights[name]\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown stacking method: {STACKING_METHOD}\")\n",
    "\n",
    "# Store stacking predictions\n",
    "test_predictions['Stacking'] = stacking_probs\n",
    "oof_predictions['Stacking'] = stacking_oof\n",
    "\n",
    "print(f\"\\nâœ… Stacking ensemble created!\")\n",
    "print(f\"   Stacking OOF AUC: {roc_auc_score(y_train_original, stacking_oof):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1cdcf3",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73ef650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ROC Curve Comparison (All Models)\n",
    "# =============================================================================\n",
    "# print(\"ROC Curve Comparison on Test Set\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# auc_comparison = compare_roc_curves(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777c824",
   "metadata": {},
   "source": [
    "### Evaluation Metrics Table\n",
    "\n",
    "Comprehensive comparison of all models with multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b5b8a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ROC-AUC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CV AUC (mean Â± std)",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Avg Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Precision (t=0.5)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall (t=0.5)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1 (t=0.5)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Optimal Threshold",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "F1 (optimal)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Recall (optimal)",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "647228f7-cd1f-4308-b89b-a29795731e45",
       "rows": [
        [
         "0",
         "LightGBM",
         "0.9014858037076173",
         "0.9381 Â± 0.0008",
         "0.5026554917303796",
         "0.2219987623762376",
         "0.7062007874015748",
         "0.3378060263653484",
         "0.4197650603738332",
         "0.28675034867503485",
         "0.7588582677165354"
        ],
        [
         "1",
         "Stacking",
         "0.9012208313816991",
         "N/A (ensemble)",
         "0.49963456457651334",
         "0.28044587977914365",
         "0.6624015748031497",
         "0.3940569421064188",
         "0.39555213238454906",
         "0.30132384904169135",
         "0.750492125984252"
        ],
        [
         "2",
         "XGBoost",
         "0.9008841561307184",
         "0.9404 Â± 0.0003",
         "0.5025311848094027",
         "0.31616721228909594",
         "0.6178641732283464",
         "0.41829085457271364",
         "0.26731033995747566",
         "0.25496701666731725",
         "0.8036417322834646"
        ],
        [
         "3",
         "CatBoost",
         "0.8905815175942591",
         "0.9182 Â± 0.0008",
         "0.47307379724145543",
         "0.15975470325330007",
         "0.7563976377952756",
         "0.2637947309705655",
         "0.47683592721583645",
         "0.2525740025740026",
         "0.7726377952755905"
        ],
        [
         "4",
         "RandomForest",
         "0.8867751014976398",
         "0.9345 Â± 0.0006",
         "0.45070348435881685",
         "0.47554196775986657",
         "0.42101377952755903",
         "0.4466196815452884",
         "0.2541171771308744",
         "0.2422015596880624",
         "0.7947834645669292"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>CV AUC (mean Â± std)</th>\n",
       "      <th>Avg Precision</th>\n",
       "      <th>Precision (t=0.5)</th>\n",
       "      <th>Recall (t=0.5)</th>\n",
       "      <th>F1 (t=0.5)</th>\n",
       "      <th>Optimal Threshold</th>\n",
       "      <th>F1 (optimal)</th>\n",
       "      <th>Recall (optimal)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.901486</td>\n",
       "      <td>0.9381 Â± 0.0008</td>\n",
       "      <td>0.502655</td>\n",
       "      <td>0.221999</td>\n",
       "      <td>0.706201</td>\n",
       "      <td>0.337806</td>\n",
       "      <td>0.419765</td>\n",
       "      <td>0.286750</td>\n",
       "      <td>0.758858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stacking</td>\n",
       "      <td>0.901221</td>\n",
       "      <td>N/A (ensemble)</td>\n",
       "      <td>0.499635</td>\n",
       "      <td>0.280446</td>\n",
       "      <td>0.662402</td>\n",
       "      <td>0.394057</td>\n",
       "      <td>0.395552</td>\n",
       "      <td>0.301324</td>\n",
       "      <td>0.750492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.900884</td>\n",
       "      <td>0.9404 Â± 0.0003</td>\n",
       "      <td>0.502531</td>\n",
       "      <td>0.316167</td>\n",
       "      <td>0.617864</td>\n",
       "      <td>0.418291</td>\n",
       "      <td>0.267310</td>\n",
       "      <td>0.254967</td>\n",
       "      <td>0.803642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.890582</td>\n",
       "      <td>0.9182 Â± 0.0008</td>\n",
       "      <td>0.473074</td>\n",
       "      <td>0.159755</td>\n",
       "      <td>0.756398</td>\n",
       "      <td>0.263795</td>\n",
       "      <td>0.476836</td>\n",
       "      <td>0.252574</td>\n",
       "      <td>0.772638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.886775</td>\n",
       "      <td>0.9345 Â± 0.0006</td>\n",
       "      <td>0.450703</td>\n",
       "      <td>0.475542</td>\n",
       "      <td>0.421014</td>\n",
       "      <td>0.446620</td>\n",
       "      <td>0.254117</td>\n",
       "      <td>0.242202</td>\n",
       "      <td>0.794783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model   ROC-AUC CV AUC (mean Â± std)  Avg Precision  \\\n",
       "0      LightGBM  0.901486     0.9381 Â± 0.0008       0.502655   \n",
       "1      Stacking  0.901221      N/A (ensemble)       0.499635   \n",
       "2       XGBoost  0.900884     0.9404 Â± 0.0003       0.502531   \n",
       "3      CatBoost  0.890582     0.9182 Â± 0.0008       0.473074   \n",
       "4  RandomForest  0.886775     0.9345 Â± 0.0006       0.450703   \n",
       "\n",
       "   Precision (t=0.5)  Recall (t=0.5)  F1 (t=0.5)  Optimal Threshold  \\\n",
       "0           0.221999        0.706201    0.337806           0.419765   \n",
       "1           0.280446        0.662402    0.394057           0.395552   \n",
       "2           0.316167        0.617864    0.418291           0.267310   \n",
       "3           0.159755        0.756398    0.263795           0.476836   \n",
       "4           0.475542        0.421014    0.446620           0.254117   \n",
       "\n",
       "   F1 (optimal)  Recall (optimal)  \n",
       "0      0.286750          0.758858  \n",
       "1      0.301324          0.750492  \n",
       "2      0.254967          0.803642  \n",
       "3      0.252574          0.772638  \n",
       "4      0.242202          0.794783  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best Model: LightGBM (ROC-AUC: 0.9015)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Comprehensive Evaluation Table\n",
    "# =============================================================================\n",
    "comparison_df = compare_models(y_test, test_predictions, threshold=0.5)\n",
    "\n",
    "# Add CV scores for base models\n",
    "cv_summary = []\n",
    "for name in comparison_df['Model']:\n",
    "    if name in cv_scores:\n",
    "        cv_summary.append(f\"{np.mean(cv_scores[name]):.4f} Â± {np.std(cv_scores[name]):.4f}\")\n",
    "    else:\n",
    "        cv_summary.append(\"N/A (ensemble)\")\n",
    "\n",
    "comparison_df['CV AUC (mean Â± std)'] = cv_summary\n",
    "\n",
    "# Reorder columns for better readability\n",
    "cols_order = ['Model', 'ROC-AUC', 'CV AUC (mean Â± std)', 'Avg Precision', \n",
    "              'Precision (t=0.5)', 'Recall (t=0.5)', 'F1 (t=0.5)', \n",
    "              'Optimal Threshold', 'F1 (optimal)', 'Recall (optimal)']\n",
    "comparison_df = comparison_df[[c for c in cols_order if c in comparison_df.columns]]\n",
    "\n",
    "print(\"=\" * 100)\n",
    "display(comparison_df)\n",
    "\n",
    "# Highlight best model\n",
    "best_model = comparison_df.iloc[0]['Model']\n",
    "best_auc = comparison_df.iloc[0]['ROC-AUC']\n",
    "print(f\"\\n Best Model: {best_model} (ROC-AUC: {best_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c835019",
   "metadata": {},
   "source": [
    "CV AUC (mean Â± std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83284dbd",
   "metadata": {},
   "source": [
    "## Comparing Stacking Ensemble Approaches\n",
    "\n",
    "* Ridge : \n",
    "    - -> ROC-AUC : 0.8772076345136302\n",
    "    - -> Avg Precision : 0.481689578620049\n",
    "    - -> Recall (optimal) : 0.7731299212598425\n",
    "    - -> F1 : 0.2679172884246429\n",
    "\n",
    "* Logistic : \n",
    "    - ->ROC-AUC : 0.8670163752759331\n",
    "    - ->Avg Precision :  0.4658784184873924\n",
    "    - ->Recall (optimal) : 0.7706692913385826\n",
    "    - -> F1 : 0.2508308973691587\n",
    "\n",
    "* Weighted : \n",
    "    - ->ROC-AUC : 0.9026547712141495\n",
    "    - ->Avg Precision : 0.4997265235104369\n",
    "    - ->Recall (optimal) : 0.7937992125984252\n",
    "    - -> F1 : 0.2640474728872519  , t = 5 olanÄ± : 0.3738330494037479\n",
    "\n",
    "t = threshold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a53fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed46c95d",
   "metadata": {},
   "source": [
    "## 6. Optional: Detailed Analysis\n",
    "\n",
    "The following sections provide additional analysis. Uncomment and run as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56970d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# [OPTIONAL] Full Evaluation Report for Best Model\n",
    "# =============================================================================\n",
    "# Uncomment below to run detailed evaluation with confusion matrix, \n",
    "# precision-recall curve, and optimal threshold analysis\n",
    "\n",
    "# best_model_name = comparison_df.iloc[0]['Model']\n",
    "# best_probs = test_predictions[best_model_name]\n",
    "\n",
    "# print(f\"Full evaluation for: {best_model_name}\")\n",
    "# report = full_evaluation_report(y_test, best_probs, model_name=best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a04c2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Feature Importance Comparison (All 4 Base Models)\n",
    "# # =============================================================================\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for idx, (model_name, model_list) in enumerate(trained_models.items()):\n",
    "#     ax = axes[idx]\n",
    "#     model = model_list[-1]  # Last fold model\n",
    "    \n",
    "#     # Get feature importances\n",
    "#     if hasattr(model, 'feature_importances_'):\n",
    "#         importances = model.feature_importances_\n",
    "#     elif hasattr(model, 'get_feature_importance'):\n",
    "#         importances = model.get_feature_importance()\n",
    "    \n",
    "#     # Create DataFrame and sort\n",
    "#     importance_df = pd.DataFrame({\n",
    "#         'Feature': filtered_features,\n",
    "#         'Importance': importances\n",
    "#     }).sort_values('Importance', ascending=True).tail(15)  # Top 15\n",
    "    \n",
    "#     # Plot horizontal bar\n",
    "#     colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(importance_df)))\n",
    "#     ax.barh(importance_df['Feature'], importance_df['Importance'], color=colors)\n",
    "#     ax.set_xlabel('Importance')\n",
    "#     ax.set_title(f'{model_name} - Top 15 Features', fontsize=12, fontweight='bold')\n",
    "#     ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# plt.suptitle('Feature Importance Comparison Across Models', fontsize=14, fontweight='bold', y=1.02)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5d8890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# [OPTIONAL] Optimal Threshold Analysis\n",
    "# =============================================================================\n",
    "# Find optimal classification threshold using different methods\n",
    "\n",
    "# best_model_name = comparison_df.iloc[0]['Model']\n",
    "# best_probs = test_predictions[best_model_name]\n",
    "# \n",
    "# # Try different threshold optimization methods\n",
    "# for method in ['youden', 'f1', 'f2']:\n",
    "#     result = find_optimal_threshold(y_test, best_probs, method=method, plot=False)\n",
    "#     print(f\"{method.upper()}: threshold={result['optimal_threshold']:.4f}, \"\n",
    "#           f\"F1={result['f1_at_threshold']:.4f}, Recall={result['recall_at_threshold']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b424f55",
   "metadata": {},
   "source": [
    "## 7. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47ff9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Save Models and Predictions (Optional)\n",
    "# =============================================================================\n",
    "import joblib\n",
    "\n",
    "# Uncomment to save:\n",
    "\n",
    "# Save all trained models\n",
    "# models_dir = ROOT / \"models\"\n",
    "# models_dir.mkdir(exist_ok=True)\n",
    "# \n",
    "# for name, model_list in trained_models.items():\n",
    "#     joblib.dump(model_list[-1], models_dir / f\"{name.lower()}_model.pkl\")\n",
    "# \n",
    "# if STACKING_METHOD in ['logistic', 'ridge']:\n",
    "#     joblib.dump(meta_model, models_dir / f\"stacking_meta_{STACKING_METHOD}.pkl\")\n",
    "#     joblib.dump(scaler, models_dir / \"stacking_scaler.pkl\")\n",
    "# \n",
    "# print(f\"âœ… Models saved to {models_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
