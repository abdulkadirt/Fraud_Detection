{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64459b7",
   "metadata": {},
   "source": [
    "# Fraud Detection - Model Training and Evaluation\n",
    "\n",
    "This notebook focuses on training and evaluating machine learning models for the IEEE-CIS Fraud Detection task.\n",
    "\n",
    "## Why Tree-Based Models?\n",
    "\n",
    "Tree-based ensemble methods are particularly well-suited for fraud detection due to:\n",
    "\n",
    "1. **Robustness to Feature Distributions**: No assumptions about normality; handles skewed transaction amounts naturally\n",
    "2. **Missing Value Handling**: Native support for missing values (especially LightGBM, XGBoost)\n",
    "3. **Feature Interactions**: Automatically captures non-linear relationships and complex interactions\n",
    "4. **Imbalanced Data**: Built-in mechanisms (`scale_pos_weight`, `is_unbalance`) for class imbalance\n",
    "5. **Correlation Insensitivity**: Unlike linear models, not affected by multicollinearity\n",
    "6. **Outlier Robustness**: Split-based decisions are less sensitive to extreme values\n",
    "7. **Mixed Feature Types**: Handles both numerical and encoded categorical features seamlessly\n",
    "\n",
    "**Models Compared**:\n",
    "1. Random Forest - Bagging ensemble, robust baseline\n",
    "2. XGBoost - Gradient boosting with regularization\n",
    "3. LightGBM - Fast gradient boosting, leaf-wise growth\n",
    "4. CatBoost - Native categorical handling, ordered boosting\n",
    "5. Stacking Ensemble - Combines base models via weighted averaging\n",
    "\n",
    "**Evaluation Strategy**: \n",
    "- K-Fold Cross-Validation on training set for model stability assessment\n",
    "- Single evaluation on temporal hold-out test set for unbiased performance\n",
    "- Primary metric: ROC-AUC (standard for fraud detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96aaa5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Import Libraries\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, auc, average_precision_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Boosting models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Custom functions\n",
    "import sys\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT / \"functions\"))\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0c973",
   "metadata": {},
   "source": [
    "## 1. Load Data & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "547fd62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded successfully!\n",
      "   Train shape: (472432, 195)\n",
      "   Test shape: (118108, 195)\n",
      "   Features for modeling: 175\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Load Preprocessed Data from EDA.ipynb\n",
    "# =============================================================================\n",
    "import pickle\n",
    "\n",
    "DATA = ROOT / \"data\"\n",
    "\n",
    "# Load DataFrames\n",
    "train_df = pd.read_parquet(DATA / \"train_preprocessed.parquet\")\n",
    "test_df = pd.read_parquet(DATA / \"test_preprocessed.parquet\")\n",
    "\n",
    "# Load feature lists\n",
    "with open(DATA / \"feature_lists.pkl\", 'rb') as f:\n",
    "    feature_lists = pickle.load(f)\n",
    "\n",
    "filtered_features = feature_lists['filtered_features']\n",
    "categorical_features = feature_lists['categorical_for_model']\n",
    "strong_features = feature_lists['strong_features']\n",
    "moderate_features = feature_lists['moderate_features']\n",
    "\n",
    "print(f\"âœ… Data loaded successfully!\")\n",
    "print(f\"   Train shape: {train_df.shape}\")\n",
    "print(f\"   Test shape: {test_df.shape}\")\n",
    "print(f\"   Features for modeling: {len(filtered_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a80d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data prepared: X_train (472432, 175), X_test (118108, 175)\n",
      "   Train fraud rate: 3.51%\n",
      "   Test fraud rate: 3.44%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Prepare Features and Target\n",
    "# =============================================================================\n",
    "X_train = train_df[filtered_features].copy()\n",
    "y_train = train_df['isFraud'].copy()\n",
    "X_test = test_df[filtered_features].copy()\n",
    "y_test = test_df['isFraud'].copy()\n",
    "\n",
    "# =============================================================================\n",
    "# Handle Infinity and Missing Values\n",
    "# =============================================================================\n",
    "inf_cols = X_train.columns[X_train.isin([np.inf, -np.inf]).any()].tolist()\n",
    "if inf_cols:\n",
    "    print(f\" Found infinity in {len(inf_cols)} columns: {inf_cols}\")\n",
    "\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "MISSING_FLAG = -999  # yarÄ±ÅŸma birincisinin uygulamasÄ±nda -999 kullanÄ±lmÄ±ÅŸ bÃ¶ylece aÄŸaÃ§ algoritmalarÄ±nda bu kayÄ±tlar direkt ayrÄ±lÄ±yor\n",
    "X_train = X_train.fillna(MISSING_FLAG)\n",
    "X_test = X_test.fillna(MISSING_FLAG)\n",
    "\n",
    "print(f\"   Data prepared: X_train {X_train.shape}, X_test {X_test.shape}\")\n",
    "print(f\"   Train fraud rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"   Test fraud rate: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b72aa",
   "metadata": {},
   "source": [
    "### Temporal Split Justification\n",
    "\n",
    "**Why Temporal vs Random Split:**\n",
    "\n",
    "In fraud detection, using a **temporal (time-based) split** is critical because:\n",
    "\n",
    "1. **Concept Drift**: Fraudsters continuously adapt their strategies over time, making historical patterns less predictive of future fraud.\n",
    "2. **Data Leakage Prevention**: Random splits would allow \"future\" information to leak into training, artificially inflating performance metrics.\n",
    "3. **Realistic Evaluation**: Models trained on past data must predict future fraud patterns, mirroring real-world deployment.\n",
    "\n",
    "**Academic Justification:**\n",
    "\n",
    "- *Krawczyk et al. (2017), \"Ensemble learning for data stream analysis: A survey\", Information Fusion*: Highlights the importance of temporal ordering in streaming data classification tasks.\n",
    "- *Dal Pozzolo et al. (2014), \"Learned lessons in credit card fraud detection from a practitioner perspective\", Expert Systems with Applications*: Demonstrates that temporal splits provide more realistic performance estimates in fraud detection.\n",
    "\n",
    "**Implication**: CV scores (computed on training data) may differ from test set performance because the test set represents a **future time period** with potentially different fraud patterns. This is expected behavior and reflects real-world deployment conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbba86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # Visualize Train/Test Split (Temporal)\n",
    "# # =============================================================================\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# # 1. Dataset Size Comparison\n",
    "# ax1 = axes[0]\n",
    "# sizes = [len(X_train), len(X_test)]\n",
    "# labels = ['Train (80%)', 'Test (20%)']\n",
    "# colors = ['#3498db', '#e74c3c']\n",
    "# bars = ax1.bar(labels, sizes, color=colors, edgecolor='black', linewidth=1.2)\n",
    "# ax1.set_ylabel('Number of Samples', fontsize=11)\n",
    "# ax1.set_title('Dataset Split', fontsize=12, fontweight='bold')\n",
    "# for bar, size in zip(bars, sizes):\n",
    "#     ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
    "#              f'{size:,}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# # 2. Fraud Rate Comparison\n",
    "# ax2 = axes[1]\n",
    "# fraud_rates = [y_train.mean()*100, y_test.mean()*100]\n",
    "# bars2 = ax2.bar(labels, fraud_rates, color=colors, edgecolor='black', linewidth=1.2)\n",
    "# ax2.set_ylabel('Fraud Rate (%)', fontsize=11)\n",
    "# ax2.set_title('Fraud Rate by Split', fontsize=12, fontweight='bold')\n",
    "# ax2.axhline(y=y_train.mean()*100, color='gray', linestyle='--', alpha=0.5, label='Train Rate')\n",
    "# for bar, rate in zip(bars2, fraud_rates):\n",
    "#     ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "#              f'{rate:.2f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# # 3. Class Distribution (Stacked)\n",
    "# ax3 = axes[2]\n",
    "# train_counts = [len(y_train) - y_train.sum(), y_train.sum()]\n",
    "# test_counts = [len(y_test) - y_test.sum(), y_test.sum()]\n",
    "# x_pos = np.arange(2)\n",
    "# width = 0.35\n",
    "\n",
    "# bars_normal = ax3.bar(x_pos - width/2, [train_counts[0], test_counts[0]], width, \n",
    "#                        label='Normal', color='#2ecc71', edgecolor='black')\n",
    "# bars_fraud = ax3.bar(x_pos + width/2, [train_counts[1], test_counts[1]], width, \n",
    "#                       label='Fraud', color='#e74c3c', edgecolor='black')\n",
    "# ax3.set_xticks(x_pos)\n",
    "# ax3.set_xticklabels(['Train', 'Test'])\n",
    "# ax3.set_ylabel('Count', fontsize=11)\n",
    "# ax3.set_title('Class Distribution', fontsize=12, fontweight='bold')\n",
    "# ax3.legend(loc='upper right')\n",
    "# ax3.set_yscale('log')  # Log scale due to imbalance\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Summary table\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"DATA SPLIT SUMMARY (Temporal Split: Train=Past, Test=Future)\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"{'Dataset':<12} {'Samples':>12} {'Normal':>12} {'Fraud':>10} {'Fraud %':>10}\")\n",
    "# print(\"-\"*60)\n",
    "# print(f\"{'Train':<12} {len(X_train):>12,} {int(len(y_train)-y_train.sum()):>12,} {int(y_train.sum()):>10,} {y_train.mean()*100:>9.2f}%\")\n",
    "# print(f\"{'Test':<12} {len(X_test):>12,} {int(len(y_test)-y_test.sum()):>12,} {int(y_test.sum()):>10,} {y_test.mean()*100:>9.2f}%\")\n",
    "# print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c445f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "455e89c0",
   "metadata": {},
   "source": [
    "## 2. Model Definitions & Stacking Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407bf2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Configuration:\n",
      "   SMOTE Method: None (using class weights)\n",
      "   Stacking Method: weighted\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Change these variables to control the pipeline\n",
    "# =============================================================================\n",
    "\n",
    "# SMOTE Configuration:\n",
    "#   None          - No resampling (use built-in class weights)\n",
    "#   'smote'       - Standard SMOTE oversampling\n",
    "#   'smote_tomek' - SMOTE + Tomek links (cleaner decision boundaries)\n",
    "\n",
    "SMOTE_METHOD = None  # <-- CHANGE THIS TO SWITCH SMOTE METHOD\n",
    "\n",
    "# Stacking Configuration:\n",
    "#   'logistic'  - Logistic Regression as meta-model (recommended for probability calibration)\n",
    "#   'ridge'     - Ridge Classifier as meta-model (more regularization)\n",
    "#   'weighted'  - No meta-model, weighted average based on CV performance\n",
    "\n",
    "STACKING_METHOD = 'weighted'  # <-- CHANGE THIS TO SWITCH STACKING METHOD\n",
    "\n",
    "print(f\"ðŸ“‹ Configuration:\")\n",
    "print(f\"   SMOTE Method: {SMOTE_METHOD if SMOTE_METHOD else 'None (using class weights)'}\")\n",
    "print(f\"   Stacking Method: {STACKING_METHOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38f0d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ Using class weights (imbalance ratio: 27.46)\n",
      "âœ… 4 base models defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Model Definitions (automatically adapts to SMOTE setting)\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate class imbalance ratio (only used when SMOTE is disabled)\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "if SMOTE_METHOD is None:\n",
    "    # Use built-in class weights when SMOTE is disabled\n",
    "    print(f\"âš–ï¸ Using class weights (imbalance ratio: {scale_pos_weight:.2f})\")\n",
    "    \n",
    "    BASE_MODELS = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            eval_metric='auc',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'LightGBM': LGBMClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            is_unbalance=True,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ),\n",
    "        'CatBoost': CatBoostClassifier(\n",
    "            iterations=300,\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            auto_class_weights='Balanced',\n",
    "            eval_metric='AUC',\n",
    "            random_seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    }\n",
    "else:\n",
    "    # No class weights needed when SMOTE balances the data\n",
    "    print(f\"âš–ï¸ SMOTE active - class weights disabled (data is balanced)\")\n",
    "    \n",
    "    BASE_MODELS = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            eval_metric='auc',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'LightGBM': LGBMClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        ),\n",
    "        'CatBoost': CatBoostClassifier(\n",
    "            iterations=300,\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            eval_metric='AUC',\n",
    "            random_seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "    }\n",
    "\n",
    "print(f\"âœ… {len(BASE_MODELS)} base models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae8680",
   "metadata": {},
   "source": [
    "### Apply SMOTE Resampling (Configured Above)\n",
    "\n",
    "The `SMOTE_METHOD` variable controls how class imbalance is handled:\n",
    "\n",
    "| Setting | Description | When to Use |\n",
    "|---------|-------------|-------------|\n",
    "| `None` | No resampling, use built-in class weights | Default, usually works well |\n",
    "| `'smote'` | Synthetic Minority Over-sampling | When class weights underperform |\n",
    "| `'smote_tomek'` | SMOTE + Tomek links cleanup | For cleaner decision boundaries |\n",
    "\n",
    "**Note:** When SMOTE is active, model class weights are automatically disabled since the data is already balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114c1664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ SMOTE disabled - using built-in class weights for imbalance handling\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Apply SMOTE (if configured)\n",
    "# =============================================================================\n",
    "\n",
    "# Store original data for reference\n",
    "X_train_original = X_train.copy()\n",
    "y_train_original = y_train.copy()\n",
    "\n",
    "if SMOTE_METHOD is not None:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    \n",
    "    print(f\"Applying {SMOTE_METHOD.upper()} to training data...\")\n",
    "    print(f\"Before: {len(X_train):,} samples (Fraud: {y_train.sum():,}, Rate: {y_train.mean()*100:.2f}%)\")\n",
    "    \n",
    "    if SMOTE_METHOD == 'smote':\n",
    "        sampler = SMOTE(\n",
    "            sampling_strategy=0.5,  # Ratio of minority to majority\n",
    "            random_state=42,\n",
    "            k_neighbors=5,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    elif SMOTE_METHOD == 'smote_tomek':\n",
    "        sampler = SMOTETomek(\n",
    "            sampling_strategy=0.5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown SMOTE method: {SMOTE_METHOD}. Use None, 'smote', or 'smote_tomek'\")\n",
    "    \n",
    "    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"After:  {len(X_train):,} samples (Fraud: {y_train.sum():,}, Rate: {y_train.mean()*100:.2f}%)\")\n",
    "    \n",
    "    # Visualize the resampling effect\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Before SMOTE\n",
    "    ax1 = axes[0]\n",
    "    counts_before = [len(y_train_original) - y_train_original.sum(), y_train_original.sum()]\n",
    "    ax1.bar(['Normal', 'Fraud'], counts_before, color=['#2ecc71', '#e74c3c'], edgecolor='black')\n",
    "    ax1.set_title('Before SMOTE', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Count')\n",
    "    for i, v in enumerate(counts_before):\n",
    "        ax1.text(i, v + 500, f'{int(v):,}', ha='center', fontweight='bold')\n",
    "    \n",
    "    # After SMOTE\n",
    "    ax2 = axes[1]\n",
    "    counts_after = [len(y_train) - y_train.sum(), y_train.sum()]\n",
    "    ax2.bar(['Normal', 'Fraud'], counts_after, color=['#2ecc71', '#e74c3c'], edgecolor='black')\n",
    "    ax2.set_title(f'After {SMOTE_METHOD.upper()}', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Count')\n",
    "    for i, v in enumerate(counts_after):\n",
    "        ax2.text(i, v + 500, f'{int(v):,}', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš™ï¸ SMOTE disabled - using built-in class weights for imbalance handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5535b5f",
   "metadata": {},
   "source": [
    "## 3. Model Training with Simplified Evaluation\n",
    "\n",
    "## Model Evaluation Methodology\n",
    "\n",
    "**Approach**: Hold-out temporal test set with K-Fold CV for model stability assessment.\n",
    "\n",
    "**Rationale**:\n",
    "- **CV on train (K=3)**: Estimates model stability and variance across different data subsets\n",
    "- **Single test evaluation**: Unbiased performance estimate on unseen future data\n",
    "- **Temporal split**: Prevents data leakage (fraudsters adapt over time)\n",
    "\n",
    "**Academic Justification**:\n",
    "- *Hastie et al. (2009), \"The Elements of Statistical Learning\", Chapter 7.10*: \"For model assessment, an independent test set is ideal. For model selection, use cross-validation.\"\n",
    "- This prevents \"peeking\" at test data during hyperparameter tuning while still providing robust stability estimates.\n",
    "\n",
    "**Metrics**:\n",
    "- **Primary**: ROC-AUC (handles class imbalance, threshold-independent)\n",
    "- **Secondary**: Average Precision (PR-AUC), optimal F1-threshold metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a4fa43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 4 models...\n",
      "Method: 3-Fold CV (stability) + Single test evaluation\n",
      "======================================================================\n",
      "RandomForest: CV AUC 0.9345 +/- 0.0006 | Test AUC 0.8845 | Time: 248.1s\n",
      "XGBoost: CV AUC 0.9404 +/- 0.0003 | Test AUC 0.8978 | Time: 1026.9s\n",
      "LightGBM: CV AUC 0.9381 +/- 0.0008 | Test AUC 0.9003 | Time: 45.7s\n",
      "CatBoost: CV AUC 0.9182 +/- 0.0008 | Test AUC 0.8933 | Time: 105.3s\n",
      "======================================================================\n",
      "Base model training complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Simplified Model Training: CV for Stability + Single Test Evaluation\n",
    "# =============================================================================\n",
    "N_FOLDS = 3\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Storage for results and trained models\n",
    "results = []\n",
    "trained_models = {}\n",
    "test_predictions = {}\n",
    "\n",
    "# Prepare SMOTE sampler if needed\n",
    "if SMOTE_METHOD is not None:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.combine import SMOTETomek\n",
    "    \n",
    "    if SMOTE_METHOD == 'smote':\n",
    "        sampler = SMOTE(sampling_strategy=0.5, random_state=42, k_neighbors=5, n_jobs=-1)\n",
    "    else:\n",
    "        sampler = SMOTETomek(sampling_strategy=0.5, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    X_train_fit, y_train_fit = sampler.fit_resample(X_train, y_train)\n",
    "    print(f\"SMOTE applied: {len(X_train_fit):,} samples (from {len(X_train):,})\")\n",
    "else:\n",
    "    X_train_fit, y_train_fit = X_train, y_train\n",
    "\n",
    "print(f\"Training {len(BASE_MODELS)} models...\")\n",
    "print(f\"Method: {N_FOLDS}-Fold CV (stability) + Single test evaluation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name, model in BASE_MODELS.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: CV for stability assessment (on original train data)\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train_fit, y_train_fit,\n",
    "        cv=skf, scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Step 2: Train ONCE on full training set\n",
    "    model.fit(X_train_fit, y_train_fit)\n",
    "    trained_models[model_name] = model\n",
    "    \n",
    "    # Step 3: Single test prediction\n",
    "    test_probs = model.predict_proba(X_test)[:, 1]\n",
    "    test_predictions[model_name] = test_probs\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    test_auc = roc_auc_score(y_test, test_probs)\n",
    "    test_ap = average_precision_score(y_test, test_probs)\n",
    "    \n",
    "    # Optimal threshold using Youden's J statistic\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, test_probs)\n",
    "    j_scores = tpr - fpr\n",
    "    optimal_idx = np.argmax(j_scores)\n",
    "    optimal_thresh = thresholds[optimal_idx]\n",
    "    \n",
    "    # Metrics at optimal threshold\n",
    "    test_pred_binary = (test_probs >= optimal_thresh).astype(int)\n",
    "    precision_opt = precision_score(y_test, test_pred_binary)\n",
    "    recall_opt = recall_score(y_test, test_pred_binary)\n",
    "    f1_opt = f1_score(y_test, test_pred_binary)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'CV_AUC_mean': cv_scores.mean(),\n",
    "        'CV_AUC_std': cv_scores.std(),\n",
    "        'Test_AUC': test_auc,\n",
    "        'Test_AP': test_ap,\n",
    "        'Optimal_Threshold': optimal_thresh,\n",
    "        'Precision_opt': precision_opt,\n",
    "        'Recall_opt': recall_opt,\n",
    "        'F1_opt': f1_opt\n",
    "    })\n",
    "    \n",
    "    print(f\"{model_name}: CV AUC {cv_scores.mean():.4f} +/- {cv_scores.std():.4f} | \"\n",
    "          f\"Test AUC {test_auc:.4f} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "# Store CV scores for stacking weights\n",
    "cv_scores_dict = {r['Model']: r['CV_AUC_mean'] for r in results}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Base model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964b6dd",
   "metadata": {},
   "source": [
    "## 4. Stacking Ensemble\n",
    "\n",
    "Combine base model predictions using the configured stacking method.\n",
    "\n",
    "**Simplified Approach**: Since we now train each model once on the full training set, stacking uses test predictions directly with weights based on CV performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b762b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking method: weighted\n",
      "Weights: {'RandomForest': 0.2505, 'XGBoost': 0.252, 'LightGBM': 0.2514, 'CatBoost': 0.2461}\n",
      "\n",
      "Stacking ensemble: Test AUC 0.9027 | AP 0.4994\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Simplified Stacking Ensemble\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Stacking method: {STACKING_METHOD}\")\n",
    "\n",
    "if STACKING_METHOD == 'weighted':\n",
    "    # Weighted average based on CV performance\n",
    "    total_auc = sum(cv_scores_dict.values())\n",
    "    weights = {name: auc_val / total_auc for name, auc_val in cv_scores_dict.items()}\n",
    "    \n",
    "    print(f\"Weights: {dict((k, round(v, 4)) for k, v in weights.items())}\")\n",
    "    \n",
    "    stacking_probs = np.zeros(len(y_test))\n",
    "    for name in BASE_MODELS:\n",
    "        stacking_probs += test_predictions[name] * weights[name]\n",
    "\n",
    "elif STACKING_METHOD in ['logistic', 'ridge']:\n",
    "    # Meta-model requires train predictions - use CV predictions\n",
    "    print(\"Note: For meta-model stacking, using weighted average as fallback\")\n",
    "    print(\"(Full OOF predictions not available in simplified pipeline)\")\n",
    "    \n",
    "    total_auc = sum(cv_scores_dict.values())\n",
    "    weights = {name: auc_val / total_auc for name, auc_val in cv_scores_dict.items()}\n",
    "    \n",
    "    stacking_probs = np.zeros(len(y_test))\n",
    "    for name in BASE_MODELS:\n",
    "        stacking_probs += test_predictions[name] * weights[name]\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown stacking method: {STACKING_METHOD}\")\n",
    "\n",
    "# Store stacking predictions\n",
    "test_predictions['Stacking'] = stacking_probs\n",
    "\n",
    "# Calculate stacking metrics\n",
    "stacking_auc = roc_auc_score(y_test, stacking_probs)\n",
    "stacking_ap = average_precision_score(y_test, stacking_probs)\n",
    "\n",
    "# Optimal threshold for stacking\n",
    "fpr, tpr, thresholds = roc_curve(y_test, stacking_probs)\n",
    "j_scores = tpr - fpr\n",
    "optimal_idx = np.argmax(j_scores)\n",
    "optimal_thresh = thresholds[optimal_idx]\n",
    "\n",
    "stacking_pred_binary = (stacking_probs >= optimal_thresh).astype(int)\n",
    "precision_opt = precision_score(y_test, stacking_pred_binary)\n",
    "recall_opt = recall_score(y_test, stacking_pred_binary)\n",
    "f1_opt = f1_score(y_test, stacking_pred_binary)\n",
    "\n",
    "# Add stacking to results\n",
    "results.append({\n",
    "    'Model': 'Stacking',\n",
    "    'CV_AUC_mean': np.nan,\n",
    "    'CV_AUC_std': np.nan,\n",
    "    'Test_AUC': stacking_auc,\n",
    "    'Test_AP': stacking_ap,\n",
    "    'Optimal_Threshold': optimal_thresh,\n",
    "    'Precision_opt': precision_opt,\n",
    "    'Recall_opt': recall_opt,\n",
    "    'F1_opt': f1_opt\n",
    "})\n",
    "\n",
    "print(f\"\\nStacking ensemble: Test AUC {stacking_auc:.4f} | AP {stacking_ap:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1cdcf3",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Comparison\n",
    "\n",
    "## Threshold Optimization\n",
    "\n",
    "**Method**: Youden's J statistic maximization\n",
    "\n",
    "The optimal classification threshold is determined by maximizing Youden's J statistic:\n",
    "\n",
    "$$J = \\text{Sensitivity} + \\text{Specificity} - 1 = \\text{TPR} - \\text{FPR}$$\n",
    "\n",
    "This method balances false positives and false negatives, providing a principled threshold that does not rely on arbitrary values (e.g., 0.5).\n",
    "\n",
    "**Academic Citation**: *Youden, W.J. (1950), \"Index for rating diagnostic tests\", Cancer, 3(1), 32-35.*\n",
    "\n",
    "**Alternative**: F1 maximization is used when precision and recall are equally important, particularly in high-imbalance scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b5b8a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "MODEL COMPARISON RESULTS\n",
      "====================================================================================================\n",
      "       Model  CV_AUC_mean  CV_AUC_std  Test_AUC  Test_AP  Optimal_Threshold  Precision_opt  Recall_opt  F1_opt\n",
      "    Stacking          NaN         NaN    0.9027   0.4994             0.3824         0.1631      0.7889  0.2703\n",
      "    LightGBM       0.9381      0.0008    0.9003   0.5042             0.4406         0.1703      0.7704  0.2789\n",
      "     XGBoost       0.9404      0.0003    0.8978   0.4889             0.3293         0.1613      0.7894  0.2679\n",
      "    CatBoost       0.9182      0.0008    0.8933   0.4800             0.4826         0.1500      0.7793  0.2515\n",
      "RandomForest       0.9345      0.0006    0.8845   0.4414             0.2560         0.1268      0.8246  0.2198\n",
      "\n",
      "Results are Excel-ready (copy from output above)\n",
      "\n",
      "Best Model: Stacking (Test ROC-AUC: 0.9027)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Final Results Table (Excel-Ready)\n",
    "# =============================================================================\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by Test_AUC descending\n",
    "results_df = results_df.sort_values('Test_AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Format numeric columns to 4 decimal places\n",
    "numeric_cols = ['CV_AUC_mean', 'CV_AUC_std', 'Test_AUC', 'Test_AP', \n",
    "                'Optimal_Threshold', 'Precision_opt', 'Recall_opt', 'F1_opt']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    results_df[col] = results_df[col].round(4)\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 100)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nResults are Excel-ready (copy from output above)\")\n",
    "\n",
    "# Identify best model\n",
    "best_model = results_df.iloc[0]['Model']\n",
    "best_auc = results_df.iloc[0]['Test_AUC']\n",
    "print(f\"\\nBest Model: {best_model} (Test ROC-AUC: {best_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46c95d",
   "metadata": {},
   "source": [
    "## 6. Optional: Feature Importance Analysis\n",
    "\n",
    "Uncomment and run the cell below to visualize feature importance across all base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04c2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# [OPTIONAL] Feature Importance Comparison (All Base Models)\n",
    "# =============================================================================\n",
    "# Uncomment to visualize feature importance\n",
    "\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "# axes = axes.flatten()\n",
    "# \n",
    "# for idx, (model_name, model) in enumerate(trained_models.items()):\n",
    "#     ax = axes[idx]\n",
    "#     \n",
    "#     if hasattr(model, 'feature_importances_'):\n",
    "#         importances = model.feature_importances_\n",
    "#     elif hasattr(model, 'get_feature_importance'):\n",
    "#         importances = model.get_feature_importance()\n",
    "#     \n",
    "#     importance_df = pd.DataFrame({\n",
    "#         'Feature': filtered_features,\n",
    "#         'Importance': importances\n",
    "#     }).sort_values('Importance', ascending=True).tail(15)\n",
    "#     \n",
    "#     colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(importance_df)))\n",
    "#     ax.barh(importance_df['Feature'], importance_df['Importance'], color=colors)\n",
    "#     ax.set_xlabel('Importance')\n",
    "#     ax.set_title(f'{model_name} - Top 15 Features', fontsize=12, fontweight='bold')\n",
    "#     ax.grid(axis='x', alpha=0.3)\n",
    "# \n",
    "# plt.suptitle('Feature Importance Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b424f55",
   "metadata": {},
   "source": [
    "## 7. Save Best Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ff9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Save Models and Results (Optional)\n",
    "# =============================================================================\n",
    "import joblib\n",
    "\n",
    "# Uncomment to save:\n",
    "\n",
    "# Save all trained models\n",
    "# models_dir = ROOT / \"models\"\n",
    "# models_dir.mkdir(exist_ok=True)\n",
    "# \n",
    "# for name, model in trained_models.items():\n",
    "#     joblib.dump(model, models_dir / f\"{name.lower()}_model.pkl\")\n",
    "# \n",
    "# print(f\"Models saved to {models_dir}\")\n",
    "\n",
    "# Save results to CSV (Excel-ready)\n",
    "# results_df.to_csv(ROOT / \"results\" / \"model_comparison.csv\", index=False)\n",
    "# print(\"Results saved to model_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
