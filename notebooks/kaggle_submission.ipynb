{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76803c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "ROOT = Path.cwd().parent\n",
    "DATA = ROOT / \"data\"\n",
    "sys.path.append(str(ROOT / \"functions\"))\n",
    "\n",
    "from categoric_functions import *\n",
    "from numeric_functions import *\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69da654",
   "metadata": {},
   "source": [
    "## 1. Load Training Data (for fitting encoders) and Kaggle Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e1cff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (590540, 434)\n",
      "Kaggle Test shape: (506691, 433)\n"
     ]
    }
   ],
   "source": [
    "# Load FULL training data (not the split version)\n",
    "train_path = DATA / \"train_merged.csv\"\n",
    "train_df = pd.read_csv(train_path, low_memory=False)\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "\n",
    "# Load Kaggle test data\n",
    "test_path = DATA / \"test_merged.csv\"\n",
    "kaggle_test = pd.read_csv(test_path, low_memory=False)\n",
    "print(f\"Kaggle Test shape: {kaggle_test.shape}\")\n",
    "\n",
    "# Save TransactionID for submission\n",
    "submission_ids = kaggle_test['TransactionID'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9119f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage decreased to 645.97 Mb (67.0% reduction)\n",
      "Memory usage decreased to 561.50 Mb (66.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "# Reduce memory\n",
    "train_df = reduce_mem_usage(train_df)\n",
    "kaggle_test = reduce_mem_usage(kaggle_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d85089",
   "metadata": {},
   "source": [
    "## 2. Apply Same Preprocessing Pipeline as EDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3c03cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 414 columns with missing values.\n",
      "There are 9 columns with missing percent > 95%\n",
      "Dropped 9 high-missing columns\n",
      "Train: (590540, 425), Test: (506691, 433)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 1: Drop high missing columns (>95%)\n",
    "# =============================================================================\n",
    "threshold = 95\n",
    "high_missing = top_missing_cols(train_df, thresh=threshold)\n",
    "cols_to_drop = high_missing[high_missing['missing_percent'] > threshold]['col'].tolist()\n",
    "\n",
    "train_df = train_df.drop(columns=cols_to_drop)\n",
    "kaggle_test = kaggle_test.drop(columns=[c for c in cols_to_drop if c in kaggle_test.columns])\n",
    "\n",
    "print(f\"Dropped {len(cols_to_drop)} high-missing columns\")\n",
    "print(f\"Train: {train_df.shape}, Test: {kaggle_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e06cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical engineering complete!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 2: Categorical Engineering Pipeline\n",
    "# =============================================================================\n",
    "def apply_categorical_engineering(df, rare_maps=None):\n",
    "    df = df.copy()\n",
    "    df, rare_maps = encode_rare_categories(\n",
    "        df, \n",
    "        columns={'card3': 200, 'card5': 300},\n",
    "        rare_maps=rare_maps\n",
    "    )\n",
    "    df = (df\n",
    "        .pipe(clean_email_domains)\n",
    "        .pipe(create_email_match)\n",
    "        .pipe(consolidate_device_info)\n",
    "        .pipe(extract_screen_features)\n",
    "    )\n",
    "    return df, rare_maps\n",
    "\n",
    "# Apply - learn from train, apply to test\n",
    "train_df, rare_maps = apply_categorical_engineering(train_df, rare_maps=None)\n",
    "kaggle_test, _ = apply_categorical_engineering(kaggle_test, rare_maps=rare_maps)\n",
    "\n",
    "print(\"Categorical engineering complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c399031b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning 171 feature pairs...\n",
      "Progress: 100/171 pairs processed...\n",
      "\n",
      "Analysis complete! Found 171 valid combinations.\n",
      "Top fraud rate: 97.6%\n",
      "Created 20 interaction features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 3: Interaction Features (use same pairs as EDA)\n",
    "# =============================================================================\n",
    "categorical_to_scan = [\n",
    "    'P_emaildomain_bin', 'R_emaildomain_bin', 'email_match', 'OS_type', 'Device_name',\n",
    "    'card5', 'card3', 'card6', 'card4', 'ProductCD', 'DeviceType', \n",
    "    'screen_width', 'screen_height', 'total_pixels', 'aspect_ratio',\n",
    "    'id_28', 'id_20', 'id_15', 'id_19'\n",
    "]\n",
    "\n",
    "# Scan on train to identify interactions\n",
    "top_combos = scan_all_bivariate_combinations(\n",
    "    train_df, \n",
    "    feature_list=[c for c in categorical_to_scan if c in train_df.columns],\n",
    "    target='isFraud',\n",
    "    min_samples=50,\n",
    "    top_n=20\n",
    ")\n",
    "\n",
    "interactions = [(row['feature1'], row['feature2']) for _, row in top_combos.iterrows()]\n",
    "\n",
    "train_df = create_interaction_features(train_df, interactions)\n",
    "kaggle_test = create_interaction_features(kaggle_test, interactions)\n",
    "\n",
    "print(f\"Created {len(interactions)} interaction features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caded1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped original transformed columns\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 4: Drop transformed columns\n",
    "# =============================================================================\n",
    "drop_cols = ['P_emaildomain', 'R_emaildomain', 'id_30', 'DeviceInfo', 'id_33']\n",
    "train_df = train_df.drop(columns=drop_cols, errors='ignore')\n",
    "kaggle_test = kaggle_test.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "print(\"Dropped original transformed columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded9b9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 22 high-card and 29 low-card features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 5: Encoding\n",
    "# =============================================================================\n",
    "# Define categorical features\n",
    "categorical_features = [\n",
    "    'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
    "    'addr1', 'addr2', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n",
    "    'DeviceType', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', \n",
    "    'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', \n",
    "    'id_27', 'id_28', 'id_29', 'id_31', 'id_32', 'id_34', 'id_35', 'id_36', \n",
    "    'id_37', 'id_38', 'P_emaildomain_bin', 'R_emaildomain_bin', 'email_match',\n",
    "    'OS_type', 'Device_name'\n",
    "]\n",
    "\n",
    "# Add interaction features\n",
    "interaction_cols = [c for c in train_df.columns if c.startswith('inter_')]\n",
    "categorical_features.extend(interaction_cols)\n",
    "\n",
    "# Get object columns that are categorical\n",
    "object_columns = [col for col in train_df.select_dtypes(include=['object']).columns \n",
    "                  if col in categorical_features]\n",
    "\n",
    "cardinality_threshold = 15\n",
    "low_card = [col for col in object_columns if train_df[col].nunique() <= cardinality_threshold]\n",
    "high_card = [col for col in object_columns if train_df[col].nunique() > cardinality_threshold]\n",
    "\n",
    "# Frequency encoding for high cardinality\n",
    "train_df, freq_maps = apply_frequency_encoding(train_df, high_card, normalize=True)\n",
    "kaggle_test, _ = apply_frequency_encoding(kaggle_test, high_card, freq_dict=freq_maps)\n",
    "\n",
    "# Label encoding for low cardinality\n",
    "train_df, label_encoders = apply_label_encoding(train_df, low_card)\n",
    "kaggle_test, _ = apply_label_encoding(kaggle_test, low_card, encoder_dict=label_encoders)\n",
    "\n",
    "print(f\"Encoded {len(high_card)} high-card and {len(low_card)} low-card features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ddea181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V columns reduced: 339 -> 15\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 6: V-column reduction\n",
    "# =============================================================================\n",
    "v_cols = [col for col in train_df.columns if col.startswith('V')]\n",
    "\n",
    "v_pattern_groups = group_by_missing_pattern(train_df, v_cols)\n",
    "representative_v = []\n",
    "\n",
    "for pattern_id, info in v_pattern_groups.items():\n",
    "    group_cols = info['columns']\n",
    "    if len(group_cols) == 1:\n",
    "        representative_v.extend(group_cols)\n",
    "        continue\n",
    "    ks_res = test_feature_discrimination(train_df, group_cols, test='ks')\n",
    "    if not ks_res.empty:\n",
    "        representative_v.append(ks_res.iloc[0]['Feature'])\n",
    "\n",
    "# Filter V columns\n",
    "cols_to_keep = train_df.columns.difference(v_cols).union(pd.Index(representative_v))\n",
    "train_df = train_df[cols_to_keep]\n",
    "kaggle_test = kaggle_test[[c for c in cols_to_keep if c in kaggle_test.columns]]\n",
    "\n",
    "print(f\"V columns reduced: {len(v_cols)} -> {len(representative_v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20c40328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (590540, 214), Test: (506691, 162)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 7: Numerical Feature Engineering\n",
    "# =============================================================================\n",
    "# Get D columns for null pattern features\n",
    "d_cols = [col for col in train_df.columns if col.startswith('D')]\n",
    "\n",
    "# Time features\n",
    "train_df = convert_dt_to_day(train_df)\n",
    "kaggle_test = convert_dt_to_day(kaggle_test)\n",
    "\n",
    "# UID creation (needs D1 in original form!)\n",
    "train_df = create_uid(train_df, uid_cols=['card1', 'addr1', 'D1'])\n",
    "kaggle_test = create_uid(kaggle_test, uid_cols=['card1', 'addr1', 'D1'])\n",
    "\n",
    "# D columns missing pattern features\n",
    "train_df = create_d_null_features(train_df, d_cols)\n",
    "kaggle_test = create_d_null_features(kaggle_test, d_cols)\n",
    "\n",
    "# D ratio and difference features\n",
    "train_df = create_d_ratio_and_diff(train_df)\n",
    "kaggle_test = create_d_ratio_and_diff(kaggle_test)\n",
    "\n",
    "# Normalize D columns (after UID creation)\n",
    "train_df = normalize_d_columns(train_df)\n",
    "kaggle_test = normalize_d_columns(kaggle_test)\n",
    "\n",
    "# Amount features\n",
    "train_df = extract_amt_decimal(train_df)\n",
    "kaggle_test = extract_amt_decimal(kaggle_test)\n",
    "\n",
    "# Log transform\n",
    "train_df['TransactionAmt_log'] = np.log1p(train_df['TransactionAmt'])\n",
    "kaggle_test['TransactionAmt_log'] = np.log1p(kaggle_test['TransactionAmt'])\n",
    "\n",
    "print(f\"Train: {train_df.shape}, Test: {kaggle_test.shape}\")\n",
    "\n",
    "# C velocity featuresprint(\"Numerical engineering complete!\")\n",
    "\n",
    "train_df = create_c_velocity_features(train_df)\n",
    "\n",
    "kaggle_test = create_c_velocity_features(kaggle_test)\n",
    "\n",
    "train_df, uid_agg_maps = create_uid_aggregations(train_df, uid_col='uid')\n",
    "# UID aggregations\n",
    "\n",
    "kaggle_test, _ = create_uid_aggregations(kaggle_test, uid_col='uid', agg_maps=uid_agg_maps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc8900fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 175 filtered features\n",
      "\n",
      "⚠️ Features missing in test: ['V222', 'V11', 'id_01', 'screen_height', 'total_pixels', 'screen_width', 'aspect_ratio', 'id_06', 'id_02', 'id_05', 'id_19', 'id_20', 'inter_R_emaildomain_bin_x_id_19', 'inter_P_emaildomain_bin_x_id_19', 'inter_card3_x_id_19', 'inter_R_emaildomain_bin_x_total_pixels', 'inter_R_emaildomain_bin_x_screen_height', 'inter_ProductCD_x_total_pixels', 'inter_aspect_ratio_x_id_20', 'inter_total_pixels_x_id_20', 'inter_ProductCD_x_screen_height', 'inter_total_pixels_x_id_15', 'inter_total_pixels_x_id_28', 'inter_screen_height_x_id_20', 'inter_screen_height_x_id_15', 'inter_screen_height_x_id_28', 'inter_R_emaildomain_bin_x_aspect_ratio', 'inter_id_20_x_id_19', 'inter_aspect_ratio_x_id_28', 'inter_aspect_ratio_x_id_15', 'inter_ProductCD_x_aspect_ratio', 'inter_email_match_x_total_pixels', 'id_31', 'inter_email_match_x_screen_height', 'inter_email_match_x_screen_width', 'id_35', 'id_15', 'id_29', 'id_28', 'id_38', 'id_36', 'id_37', 'inter_card6_x_total_pixels', 'inter_card6_x_screen_height', 'id_12', 'inter_card6_x_screen_width', 'id_17', 'id_16', 'inter_card6_x_aspect_ratio', 'inter_P_emaildomain_bin_x_screen_height', 'inter_P_emaildomain_bin_x_total_pixels', 'inter_P_emaildomain_bin_x_aspect_ratio', 'id_13', 'id_32', 'id_18', 'id_14', 'OS_type']\n",
      "Using 118 available features\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Step 8: Load filtered_features from saved pickle\n",
    "# =============================================================================\n",
    "with open(DATA / \"feature_lists.pkl\", 'rb') as f:\n",
    "    feature_lists = pickle.load(f)\n",
    "\n",
    "filtered_features = feature_lists['filtered_features']\n",
    "print(f\"Using {len(filtered_features)} filtered features\")\n",
    "\n",
    "# Check which features are available in both datasets\n",
    "available_features = [f for f in filtered_features if f in train_df.columns and f in kaggle_test.columns]\n",
    "missing_in_test = [f for f in filtered_features if f not in kaggle_test.columns]\n",
    "\n",
    "if missing_in_test:\n",
    "    print(f\"\\n⚠️ Features missing in test: {missing_in_test}\")\n",
    "    print(f\"Using {len(available_features)} available features\")\n",
    "else:\n",
    "    print(f\"\\n✅ All {len(filtered_features)} features available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1020409",
   "metadata": {},
   "source": [
    "## 3. Train Models with Stacking Ensemble\n",
    "\n",
    "Using the same approach as modeling.ipynb - train all base models and combine with weighted stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "145a8a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fraud rate: 3.50%\n",
      "X_kaggle_test: (506691, 118)\n",
      "X_train: (590540, 118)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Prepare data\n",
    "X_train = train_df[available_features].copy()\n",
    "y_train = train_df['isFraud'].copy()\n",
    "X_kaggle_test = kaggle_test[available_features].copy()\n",
    "\n",
    "# Handle infinity and missing values\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_kaggle_test = X_kaggle_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "MISSING_FLAG = -222  # Same as modeling.ipynb\n",
    "\n",
    "X_train = X_train.fillna(MISSING_FLAG)\n",
    "print(f\"Training fraud rate: {y_train.mean()*100:.2f}%\")\n",
    "\n",
    "X_kaggle_test = X_kaggle_test.fillna(MISSING_FLAG)\n",
    "print(f\"X_kaggle_test: {X_kaggle_test.shape}\")\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ba62d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance ratio: 27.58\n",
      "\n",
      "▶ Training RandomForest...\n",
      "   Training time: 99.03 seconds\n",
      "\n",
      "▶ Training XGBoost...\n",
      "   Training time: 169.37 seconds\n",
      "\n",
      "▶ Training LightGBM...\n",
      "   Training time: 17.88 seconds\n",
      "\n",
      "▶ Training CatBoost...\n",
      "   Training time: 52.17 seconds\n",
      "\n",
      "✅ All models trained in 338.45 seconds\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Train All Base Models (same config as modeling.ipynb)\n",
    "# =============================================================================\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Class imbalance ratio: {scale_pos_weight:.2f}\")\n",
    "\n",
    "BASE_MODELS = {\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='auc',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        is_unbalance=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        iterations=300,\n",
    "        depth=6,\n",
    "        learning_rate=0.05,\n",
    "        auto_class_weights='Balanced',\n",
    "        eval_metric='AUC',\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Train each model and get predictions\n",
    "test_predictions = {}\n",
    "total_start = time.time()\n",
    "for name, model in BASE_MODELS.items():\n",
    "    print(f\"\\n▶ Training {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    test_predictions[name] = model.predict_proba(X_kaggle_test)[:, 1]\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"   Training time: {elapsed:.2f} seconds\")\n",
    "total_elapsed = time.time() - total_start\n",
    "print(f\"\\n✅ All models trained in {total_elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a8d02",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions with Weighted Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3783e4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking weights:\n",
      "  RandomForest: 0.20\n",
      "  XGBoost: 0.25\n",
      "  LightGBM: 0.30\n",
      "  CatBoost: 0.25\n",
      "\n",
      "Predictions shape: (506691,)\n",
      "Prediction stats:\n",
      "  Min: 0.0089\n",
      "  Max: 0.9966\n",
      "  Mean: 0.2275\n",
      "✅ Submission file saved to: c:\\Users\\Abdulkadir\\Desktop\\Uygulama çalışmaları\\Fraud_Detection\\Fraud_Detection\\data\\submission.csv\n",
      "\n",
      "Submission preview:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TransactionID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "isFraud",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "60609466-0954-479f-8301-c29f1cac379b",
       "rows": [
        [
         "0",
         "3663549",
         "0.03510389457784191"
        ],
        [
         "1",
         "3663550",
         "0.08627323296069712"
        ],
        [
         "2",
         "3663551",
         "0.23677987609978693"
        ],
        [
         "3",
         "3663552",
         "0.08930602943490845"
        ],
        [
         "4",
         "3663553",
         "0.09611010209988094"
        ],
        [
         "5",
         "3663554",
         "0.147496127090478"
        ],
        [
         "6",
         "3663555",
         "0.28868264726878556"
        ],
        [
         "7",
         "3663556",
         "0.2994947403279088"
        ],
        [
         "8",
         "3663557",
         "0.038300631714801914"
        ],
        [
         "9",
         "3663558",
         "0.257556233531799"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>0.035104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>0.086273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>0.236780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>0.089306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>0.096110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3663554</td>\n",
       "      <td>0.147496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3663555</td>\n",
       "      <td>0.288683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3663556</td>\n",
       "      <td>0.299495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3663557</td>\n",
       "      <td>0.038301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3663558</td>\n",
       "      <td>0.257556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID   isFraud\n",
       "0        3663549  0.035104\n",
       "1        3663550  0.086273\n",
       "2        3663551  0.236780\n",
       "3        3663552  0.089306\n",
       "4        3663553  0.096110\n",
       "5        3663554  0.147496\n",
       "6        3663555  0.288683\n",
       "7        3663556  0.299495\n",
       "8        3663557  0.038301\n",
       "9        3663558  0.257556"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape: (506691, 2)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Weighted Stacking (best method from modeling.ipynb experiments)\n",
    "# =============================================================================\n",
    "# Using equal weights since we don't have CV scores on full training data\n",
    "# You can adjust these weights based on modeling.ipynb results\n",
    "\n",
    "weights = {\n",
    "    'RandomForest': 0.20,\n",
    "    'XGBoost': 0.25,\n",
    "    'LightGBM': 0.30,\n",
    "    'CatBoost': 0.25\n",
    "}\n",
    "\n",
    "print(\"Stacking weights:\")\n",
    "for name, weight in weights.items():\n",
    "    print(f\"  {name}: {weight:.2f}\")\n",
    "\n",
    "# Weighted average of predictions\n",
    "predictions = np.zeros(len(X_kaggle_test))\n",
    "for name in BASE_MODELS:\n",
    "    predictions += test_predictions[name] * weights[name]\n",
    "\n",
    "print(f\"\\nPredictions shape: {predictions.shape}\")\n",
    "print(f\"Prediction stats:\")\n",
    "print(f\"  Min: {predictions.min():.4f}\")\n",
    "print(f\"  Max: {predictions.max():.4f}\")\n",
    "print(f\"  Mean: {predictions.mean():.4f}\")\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'TransactionID': submission_ids,\n",
    "    'isFraud': predictions\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission_path = DATA / \"submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"✅ Submission file saved to: {submission_path}\")\n",
    "print(f\"\\nSubmission preview:\")\n",
    "display(submission.head(10))\n",
    "print(f\"\\nShape: {submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0be335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance ratio: 27.58\n",
      "\n",
      "▶ Training LightGBM...\n",
      "   Training time: 14.33 seconds\n",
      "\n",
      "Predictions shape: (506691,)\n",
      "Prediction stats:\n",
      "  Min: 0.0010\n",
      "  Max: 0.9983\n",
      "  Mean: 0.2365\n",
      "✅ Submission file saved to: c:\\Users\\Abdulkadir\\Desktop\\Uygulama çalışmaları\\Fraud_Detection\\Fraud_Detection\\data\\lgbm_submission.csv\n",
      "\n",
      "Submission preview:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TransactionID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "isFraud",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "880f7aa8-91bf-45f5-bc24-9ed4f20c4e38",
       "rows": [
        [
         "0",
         "3663549",
         "0.014269787599029454"
        ],
        [
         "1",
         "3663550",
         "0.07741641145879744"
        ],
        [
         "2",
         "3663551",
         "0.34232882070223997"
        ],
        [
         "3",
         "3663552",
         "0.08151522035463357"
        ],
        [
         "4",
         "3663553",
         "0.0989054863717891"
        ],
        [
         "5",
         "3663554",
         "0.1378896928332072"
        ],
        [
         "6",
         "3663555",
         "0.346744583661986"
        ],
        [
         "7",
         "3663556",
         "0.29944807490547243"
        ],
        [
         "8",
         "3663557",
         "0.017299258365012414"
        ],
        [
         "9",
         "3663558",
         "0.20430792311064222"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>0.014270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>0.077416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>0.342329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>0.081515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>0.098905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3663554</td>\n",
       "      <td>0.137890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3663555</td>\n",
       "      <td>0.346745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3663556</td>\n",
       "      <td>0.299448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3663557</td>\n",
       "      <td>0.017299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3663558</td>\n",
       "      <td>0.204308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID   isFraud\n",
       "0        3663549  0.014270\n",
       "1        3663550  0.077416\n",
       "2        3663551  0.342329\n",
       "3        3663552  0.081515\n",
       "4        3663553  0.098905\n",
       "5        3663554  0.137890\n",
       "6        3663555  0.346745\n",
       "7        3663556  0.299448\n",
       "8        3663557  0.017299\n",
       "9        3663558  0.204308"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape: (506691, 2)\n"
     ]
    }
   ],
   "source": [
    "# # =============================================================================\n",
    "# # Alternative: Train and Use Only LightGBM\n",
    "# # =============================================================================\n",
    "# scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "# print(f\"Class imbalance ratio: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# # Define only LightGBM model with same config\n",
    "# lgbm_model = LGBMClassifier(\n",
    "#     n_estimators=300,\n",
    "#     max_depth=8,\n",
    "#     learning_rate=0.05,\n",
    "#     num_leaves=31,\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     is_unbalance=True,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1,\n",
    "#     verbose=-1\n",
    "# )\n",
    "\n",
    "# # Train LightGBM\n",
    "# print(\"\\n▶ Training LightGBM...\")\n",
    "# start_time = time.time()\n",
    "# lgbm_model.fit(X_train, y_train)\n",
    "# predictions = lgbm_model.predict_proba(X_kaggle_test)[:, 1]\n",
    "# elapsed = time.time() - start_time\n",
    "# print(f\"   Training time: {elapsed:.2f} seconds\")\n",
    "\n",
    "# print(f\"\\nPredictions shape: {predictions.shape}\")\n",
    "# print(f\"Prediction stats:\")\n",
    "# print(f\"  Min: {predictions.min():.4f}\")\n",
    "# print(f\"  Max: {predictions.max():.4f}\")\n",
    "# print(f\"  Mean: {predictions.mean():.4f}\")\n",
    "\n",
    "# # Create submission file\n",
    "# submission = pd.DataFrame({\n",
    "#     'TransactionID': submission_ids,\n",
    "#     'isFraud': predictions\n",
    "# })\n",
    "\n",
    "# # Save submission\n",
    "# submission_path = DATA / \"lgbm_submission.csv\"\n",
    "# submission.to_csv(submission_path, index=False)\n",
    "\n",
    "# print(f\"✅ Submission file saved to: {submission_path}\")\n",
    "# print(f\"\\nSubmission preview:\")\n",
    "# display(submission.head(10))\n",
    "# print(f\"\\nShape: {submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38f64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
